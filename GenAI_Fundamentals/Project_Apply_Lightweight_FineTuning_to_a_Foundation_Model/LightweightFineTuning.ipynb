{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fb3ff",
   "metadata": {},
   "source": [
    "TODO: In this cell, describe your choices for each of the following\n",
    "\n",
    "* PEFT technique:  LoRA (Low-Rank Adaptation)\n",
    "* Model: DistilBERT (distilbert-base-uncased), chosen for its lightweight structure and compatibility with sequence classification tasks.\n",
    "* Evaluation approach:  sklearn.metrics with accuracy, precision, recall, and F1-score \n",
    "* Fine-tuning dataset: fancyzhx/amazon_polarity Amazon Polarity Dataset (from Hugging Face's datasets library),"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d76bb",
   "metadata": {},
   "source": [
    "## Loading and Evaluating a Foundation Model\n",
    "\n",
    "TODO: In the cells below, load your chosen pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f551c63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries (Step 1)\n",
    "from datasets import load_dataset\n",
    "from transformers import DistilBertForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import torch\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f21a0f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset (Step 2)\n",
    "dataset = load_dataset(\"fancyzhx/amazon_polarity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4935cb4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained model and tokenizer (Step 3)\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "id2label = {0: \"negative\", 1: \"positive\"}\n",
    "label2id = {\"negative\": 0, \"positive\": 1}\n",
    "\n",
    "base_model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    model_name, num_labels=2, id2label=id2label, label2id=label2id\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93a6d882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertForSequenceClassification(\n",
      "  (distilbert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x TransformerBlock(\n",
      "          (attention): DistilBertSdpaAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71d3c623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all parameters in the base model (Step 4)\n",
    "for name, param in base_model.named_parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d457837f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 1, 'title': 'Amazing!', 'content': 'This soundtrack is my favorite music of all time, hands down. The intense sadness of \"Prisoners of Fate\" (which means all the more if you\\'ve played the game) and the hope in \"A Distant Promise\" and \"Girl who Stole the Star\" have been an important inspiration to me personally throughout my teen years. The higher energy tracks like \"Chrono Cross ~ Time\\'s Scar~\", \"Time of the Dreamwatch\", and \"Chronomantique\" (indefinably remeniscent of Chrono Trigger) are all absolutely superb as well.This soundtrack is amazing music, probably the best of this composer\\'s work (I haven\\'t heard the Xenogears soundtrack, so I can\\'t say for sure), and even if you\\'ve never played the game, it would be worth twice the price to buy it.I wish I could give it 6 stars.'}\n"
     ]
    }
   ],
   "source": [
    "#An example of data row\n",
    "print(dataset['train'][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7cb6043b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the training dataset: 3600000\n",
      "Number of rows in the test dataset: 400000\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of rows in the training dataset:\", len(dataset['train']))\n",
    "print(\"Number of rows in the test dataset:\", len(dataset['test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f28c4a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the dataset (Step 5)\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['content'], truncation=True, padding=True, max_length=256)\n",
    "\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "019b9f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train and test sets (Step 6) Not: Because the data too much, I only used 100000 data for training and 50000 data for testing\n",
    "train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(100000))\n",
    "test_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(50000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f272cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Determine the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb3637e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): DistilBertSdpaAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Move the model to the appropriate device (Step 7)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "base_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a61058a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable Parameters:\n"
     ]
    }
   ],
   "source": [
    "# Ensure if there is any trainable parameters\n",
    "print(\"Trainable Parameters:\")\n",
    "for name, param in base_model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5176b07f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the base model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cdcb118260a4fce85b20fa9cb20e159",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Model Results: {'eval_loss': 0.693297266960144, 'eval_model_preparation_time': 0.001, 'eval_accuracy': 0.5036, 'eval_precision': 0.5036159515118122, 'eval_recall': 0.8727272727272727, 'eval_f1': 0.6386769929540558, 'eval_runtime': 75.1195, 'eval_samples_per_second': 665.607, 'eval_steps_per_second': 41.6}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the foundation model (Step 8)\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = torch.argmax(torch.tensor(predictions), dim=-1).numpy()\n",
    "    labels = torch.tensor(labels).numpy()\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average=\"binary\")\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results_base_v1\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_total_limit=1\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=base_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "print(\"Evaluating the base model...\")\n",
    "base_results = trainer.evaluate()\n",
    "print(\"Base Model Results:\", base_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52a229",
   "metadata": {},
   "source": [
    "## Performing Parameter-Efficient Fine-Tuning\n",
    "\n",
    "TODO: In the cells below, create a PEFT model from your loaded model, run a training loop, and save the PEFT model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f4fb786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertForSequenceClassification(\n",
      "  (distilbert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x TransformerBlock(\n",
      "          (attention): DistilBertSdpaAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5775fadf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,181,954 || all params: 68,136,964 || trainable%: 1.7347\n"
     ]
    }
   ],
   "source": [
    "# Configure LoRA for PEFT (Step 9)\n",
    "config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    target_modules=[\"q_lin\", \"v_lin\"],\n",
    "    lora_dropout=0.05,\n",
    "    task_type=TaskType.SEQ_CLS\n",
    ")\n",
    "peft_model = get_peft_model(base_model, config)\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "894046c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f4eee449db2484991f2c3262cb7a59b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6823, 'grad_norm': 1.0510802268981934, 'learning_rate': 4.992e-05, 'epoch': 0.0}\n",
      "{'loss': 0.673, 'grad_norm': 1.4424479007720947, 'learning_rate': 4.9840000000000004e-05, 'epoch': 0.0}\n",
      "{'loss': 0.6633, 'grad_norm': 1.745947241783142, 'learning_rate': 4.976e-05, 'epoch': 0.0}\n",
      "{'loss': 0.6414, 'grad_norm': 1.2828929424285889, 'learning_rate': 4.9680000000000005e-05, 'epoch': 0.01}\n",
      "{'loss': 0.6389, 'grad_norm': 2.2771382331848145, 'learning_rate': 4.96e-05, 'epoch': 0.01}\n",
      "{'loss': 0.5808, 'grad_norm': 1.2292017936706543, 'learning_rate': 4.952e-05, 'epoch': 0.01}\n",
      "{'loss': 0.5011, 'grad_norm': 1.5200871229171753, 'learning_rate': 4.944e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3836, 'grad_norm': 2.2602458000183105, 'learning_rate': 4.936e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3464, 'grad_norm': 2.9825305938720703, 'learning_rate': 4.928e-05, 'epoch': 0.01}\n",
      "{'loss': 0.3419, 'grad_norm': 3.7818498611450195, 'learning_rate': 4.92e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3288, 'grad_norm': 1.7554080486297607, 'learning_rate': 4.9120000000000004e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3475, 'grad_norm': 1.2734286785125732, 'learning_rate': 4.9040000000000005e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3219, 'grad_norm': 4.9945502281188965, 'learning_rate': 4.896e-05, 'epoch': 0.02}\n",
      "{'loss': 0.3815, 'grad_norm': 2.4009299278259277, 'learning_rate': 4.8880000000000006e-05, 'epoch': 0.02}\n",
      "{'loss': 0.21, 'grad_norm': 2.3717453479766846, 'learning_rate': 4.88e-05, 'epoch': 0.02}\n",
      "{'loss': 0.4176, 'grad_norm': 1.626600742340088, 'learning_rate': 4.872000000000001e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3995, 'grad_norm': 2.811142683029175, 'learning_rate': 4.864e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3605, 'grad_norm': 4.267920017242432, 'learning_rate': 4.856e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3284, 'grad_norm': 2.495117664337158, 'learning_rate': 4.8480000000000003e-05, 'epoch': 0.03}\n",
      "{'loss': 0.2603, 'grad_norm': 2.2556962966918945, 'learning_rate': 4.8400000000000004e-05, 'epoch': 0.03}\n",
      "{'loss': 0.3788, 'grad_norm': 3.3713924884796143, 'learning_rate': 4.8320000000000005e-05, 'epoch': 0.03}\n",
      "{'loss': 0.2682, 'grad_norm': 3.7929153442382812, 'learning_rate': 4.824e-05, 'epoch': 0.04}\n",
      "{'loss': 0.3104, 'grad_norm': 3.049060583114624, 'learning_rate': 4.816e-05, 'epoch': 0.04}\n",
      "{'loss': 0.3248, 'grad_norm': 3.231020212173462, 'learning_rate': 4.808e-05, 'epoch': 0.04}\n",
      "{'loss': 0.3098, 'grad_norm': 1.1739230155944824, 'learning_rate': 4.8e-05, 'epoch': 0.04}\n",
      "{'loss': 0.2482, 'grad_norm': 2.3531553745269775, 'learning_rate': 4.792e-05, 'epoch': 0.04}\n",
      "{'loss': 0.3286, 'grad_norm': 4.183407783508301, 'learning_rate': 4.784e-05, 'epoch': 0.04}\n",
      "{'loss': 0.281, 'grad_norm': 5.832273960113525, 'learning_rate': 4.7760000000000004e-05, 'epoch': 0.04}\n",
      "{'loss': 0.2611, 'grad_norm': 1.8811089992523193, 'learning_rate': 4.7680000000000004e-05, 'epoch': 0.05}\n",
      "{'loss': 0.227, 'grad_norm': 1.5885905027389526, 'learning_rate': 4.76e-05, 'epoch': 0.05}\n",
      "{'loss': 0.3279, 'grad_norm': 1.4576199054718018, 'learning_rate': 4.7520000000000006e-05, 'epoch': 0.05}\n",
      "{'loss': 0.2742, 'grad_norm': 2.599059581756592, 'learning_rate': 4.744e-05, 'epoch': 0.05}\n",
      "{'loss': 0.2792, 'grad_norm': 3.719454050064087, 'learning_rate': 4.736000000000001e-05, 'epoch': 0.05}\n",
      "{'loss': 0.2835, 'grad_norm': 4.440554618835449, 'learning_rate': 4.728e-05, 'epoch': 0.05}\n",
      "{'loss': 0.3825, 'grad_norm': 2.5985825061798096, 'learning_rate': 4.72e-05, 'epoch': 0.06}\n",
      "{'loss': 0.3038, 'grad_norm': 5.0021233558654785, 'learning_rate': 4.712e-05, 'epoch': 0.06}\n",
      "{'loss': 0.3001, 'grad_norm': 2.121096134185791, 'learning_rate': 4.7040000000000004e-05, 'epoch': 0.06}\n",
      "{'loss': 0.2754, 'grad_norm': 1.3921852111816406, 'learning_rate': 4.6960000000000004e-05, 'epoch': 0.06}\n",
      "{'loss': 0.2399, 'grad_norm': 3.79490065574646, 'learning_rate': 4.688e-05, 'epoch': 0.06}\n",
      "{'loss': 0.2841, 'grad_norm': 4.618133544921875, 'learning_rate': 4.6800000000000006e-05, 'epoch': 0.06}\n",
      "{'loss': 0.3724, 'grad_norm': 2.718674659729004, 'learning_rate': 4.672e-05, 'epoch': 0.07}\n",
      "{'loss': 0.3052, 'grad_norm': 5.106308937072754, 'learning_rate': 4.664e-05, 'epoch': 0.07}\n",
      "{'loss': 0.2615, 'grad_norm': 3.1032462120056152, 'learning_rate': 4.656e-05, 'epoch': 0.07}\n",
      "{'loss': 0.3165, 'grad_norm': 3.1476523876190186, 'learning_rate': 4.648e-05, 'epoch': 0.07}\n",
      "{'loss': 0.2992, 'grad_norm': 2.3772335052490234, 'learning_rate': 4.64e-05, 'epoch': 0.07}\n",
      "{'loss': 0.2197, 'grad_norm': 2.8687024116516113, 'learning_rate': 4.6320000000000004e-05, 'epoch': 0.07}\n",
      "{'loss': 0.2997, 'grad_norm': 3.791005849838257, 'learning_rate': 4.624e-05, 'epoch': 0.08}\n",
      "{'loss': 0.2248, 'grad_norm': 2.4059369564056396, 'learning_rate': 4.6160000000000005e-05, 'epoch': 0.08}\n",
      "{'loss': 0.3183, 'grad_norm': 4.468955039978027, 'learning_rate': 4.608e-05, 'epoch': 0.08}\n",
      "{'loss': 0.2357, 'grad_norm': 3.0182223320007324, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.08}\n",
      "{'loss': 0.3509, 'grad_norm': 2.5738487243652344, 'learning_rate': 4.592e-05, 'epoch': 0.08}\n",
      "{'loss': 0.2693, 'grad_norm': 2.747986078262329, 'learning_rate': 4.584e-05, 'epoch': 0.08}\n",
      "{'loss': 0.2244, 'grad_norm': 3.9922289848327637, 'learning_rate': 4.576e-05, 'epoch': 0.08}\n",
      "{'loss': 0.2555, 'grad_norm': 4.173553943634033, 'learning_rate': 4.568e-05, 'epoch': 0.09}\n",
      "{'loss': 0.2751, 'grad_norm': 0.5221723914146423, 'learning_rate': 4.5600000000000004e-05, 'epoch': 0.09}\n",
      "{'loss': 0.1963, 'grad_norm': 2.1199278831481934, 'learning_rate': 4.5520000000000005e-05, 'epoch': 0.09}\n",
      "{'loss': 0.3347, 'grad_norm': 1.9010071754455566, 'learning_rate': 4.5440000000000005e-05, 'epoch': 0.09}\n",
      "{'loss': 0.2452, 'grad_norm': 1.2753843069076538, 'learning_rate': 4.536e-05, 'epoch': 0.09}\n",
      "{'loss': 0.286, 'grad_norm': 4.606944561004639, 'learning_rate': 4.528e-05, 'epoch': 0.09}\n",
      "{'loss': 0.3677, 'grad_norm': 2.2290449142456055, 'learning_rate': 4.52e-05, 'epoch': 0.1}\n",
      "{'loss': 0.1783, 'grad_norm': 3.2317559719085693, 'learning_rate': 4.512e-05, 'epoch': 0.1}\n",
      "{'loss': 0.2132, 'grad_norm': 3.1926310062408447, 'learning_rate': 4.504e-05, 'epoch': 0.1}\n",
      "{'loss': 0.1992, 'grad_norm': 2.46124529838562, 'learning_rate': 4.496e-05, 'epoch': 0.1}\n",
      "{'loss': 0.2618, 'grad_norm': 2.591102361679077, 'learning_rate': 4.488e-05, 'epoch': 0.1}\n",
      "{'loss': 0.3347, 'grad_norm': 2.9041736125946045, 'learning_rate': 4.4800000000000005e-05, 'epoch': 0.1}\n",
      "{'loss': 0.2995, 'grad_norm': 2.2775261402130127, 'learning_rate': 4.472e-05, 'epoch': 0.11}\n",
      "{'loss': 0.1939, 'grad_norm': 2.601229190826416, 'learning_rate': 4.4640000000000006e-05, 'epoch': 0.11}\n",
      "{'loss': 0.2976, 'grad_norm': 3.4596304893493652, 'learning_rate': 4.456e-05, 'epoch': 0.11}\n",
      "{'loss': 0.2659, 'grad_norm': 2.858276128768921, 'learning_rate': 4.448e-05, 'epoch': 0.11}\n",
      "{'loss': 0.3162, 'grad_norm': 5.735630035400391, 'learning_rate': 4.44e-05, 'epoch': 0.11}\n",
      "{'loss': 0.3262, 'grad_norm': 1.801256775856018, 'learning_rate': 4.432e-05, 'epoch': 0.11}\n",
      "{'loss': 0.2731, 'grad_norm': 1.7432526350021362, 'learning_rate': 4.424e-05, 'epoch': 0.12}\n",
      "{'loss': 0.2647, 'grad_norm': 3.479940176010132, 'learning_rate': 4.4160000000000004e-05, 'epoch': 0.12}\n",
      "{'loss': 0.2015, 'grad_norm': 1.662420630455017, 'learning_rate': 4.4080000000000005e-05, 'epoch': 0.12}\n",
      "{'loss': 0.2762, 'grad_norm': 3.1178882122039795, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.12}\n",
      "{'loss': 0.2389, 'grad_norm': 2.062509298324585, 'learning_rate': 4.392e-05, 'epoch': 0.12}\n",
      "{'loss': 0.2591, 'grad_norm': 4.314846038818359, 'learning_rate': 4.384e-05, 'epoch': 0.12}\n",
      "{'loss': 0.2497, 'grad_norm': 3.591141939163208, 'learning_rate': 4.376e-05, 'epoch': 0.12}\n",
      "{'loss': 0.2248, 'grad_norm': 3.654670000076294, 'learning_rate': 4.368e-05, 'epoch': 0.13}\n",
      "{'loss': 0.2453, 'grad_norm': 3.106750726699829, 'learning_rate': 4.36e-05, 'epoch': 0.13}\n",
      "{'loss': 0.1801, 'grad_norm': 2.927663803100586, 'learning_rate': 4.352e-05, 'epoch': 0.13}\n",
      "{'loss': 0.3427, 'grad_norm': 1.5037577152252197, 'learning_rate': 4.3440000000000004e-05, 'epoch': 0.13}\n",
      "{'loss': 0.2619, 'grad_norm': 1.8495361804962158, 'learning_rate': 4.336e-05, 'epoch': 0.13}\n",
      "{'loss': 0.3049, 'grad_norm': 4.172956943511963, 'learning_rate': 4.3280000000000006e-05, 'epoch': 0.13}\n",
      "{'loss': 0.2394, 'grad_norm': 2.3365743160247803, 'learning_rate': 4.32e-05, 'epoch': 0.14}\n",
      "{'loss': 0.2425, 'grad_norm': 1.3392823934555054, 'learning_rate': 4.312000000000001e-05, 'epoch': 0.14}\n",
      "{'loss': 0.2996, 'grad_norm': 6.249221324920654, 'learning_rate': 4.304e-05, 'epoch': 0.14}\n",
      "{'loss': 0.2261, 'grad_norm': 2.4813826084136963, 'learning_rate': 4.296e-05, 'epoch': 0.14}\n",
      "{'loss': 0.2801, 'grad_norm': 3.0430991649627686, 'learning_rate': 4.288e-05, 'epoch': 0.14}\n",
      "{'loss': 0.2289, 'grad_norm': 1.813239336013794, 'learning_rate': 4.2800000000000004e-05, 'epoch': 0.14}\n",
      "{'loss': 0.2221, 'grad_norm': 2.4715518951416016, 'learning_rate': 4.2720000000000004e-05, 'epoch': 0.15}\n",
      "{'loss': 0.2873, 'grad_norm': 3.008429765701294, 'learning_rate': 4.2640000000000005e-05, 'epoch': 0.15}\n",
      "{'loss': 0.3082, 'grad_norm': 3.0677621364593506, 'learning_rate': 4.256e-05, 'epoch': 0.15}\n",
      "{'loss': 0.2201, 'grad_norm': 2.233306646347046, 'learning_rate': 4.248e-05, 'epoch': 0.15}\n",
      "{'loss': 0.1738, 'grad_norm': 2.567291259765625, 'learning_rate': 4.24e-05, 'epoch': 0.15}\n",
      "{'loss': 0.3151, 'grad_norm': 1.7926610708236694, 'learning_rate': 4.232e-05, 'epoch': 0.15}\n",
      "{'loss': 0.2023, 'grad_norm': 2.9601237773895264, 'learning_rate': 4.224e-05, 'epoch': 0.16}\n",
      "{'loss': 0.1601, 'grad_norm': 3.252981662750244, 'learning_rate': 4.2159999999999996e-05, 'epoch': 0.16}\n",
      "{'loss': 0.3144, 'grad_norm': 4.623220443725586, 'learning_rate': 4.2080000000000004e-05, 'epoch': 0.16}\n",
      "{'loss': 0.2575, 'grad_norm': 1.959507942199707, 'learning_rate': 4.2e-05, 'epoch': 0.16}\n",
      "{'loss': 0.2637, 'grad_norm': 3.0721845626831055, 'learning_rate': 4.1920000000000005e-05, 'epoch': 0.16}\n",
      "{'loss': 0.1947, 'grad_norm': 1.5658559799194336, 'learning_rate': 4.184e-05, 'epoch': 0.16}\n",
      "{'loss': 0.2667, 'grad_norm': 3.988027334213257, 'learning_rate': 4.176000000000001e-05, 'epoch': 0.16}\n",
      "{'loss': 0.2007, 'grad_norm': 1.9798156023025513, 'learning_rate': 4.168e-05, 'epoch': 0.17}\n",
      "{'loss': 0.2461, 'grad_norm': 2.0097224712371826, 'learning_rate': 4.16e-05, 'epoch': 0.17}\n",
      "{'loss': 0.2069, 'grad_norm': 5.817809581756592, 'learning_rate': 4.152e-05, 'epoch': 0.17}\n",
      "{'loss': 0.3097, 'grad_norm': 3.1651155948638916, 'learning_rate': 4.144e-05, 'epoch': 0.17}\n",
      "{'loss': 0.355, 'grad_norm': 3.4217746257781982, 'learning_rate': 4.1360000000000004e-05, 'epoch': 0.17}\n",
      "{'loss': 0.2976, 'grad_norm': 3.368281602859497, 'learning_rate': 4.1280000000000005e-05, 'epoch': 0.17}\n",
      "{'loss': 0.2637, 'grad_norm': 1.6563260555267334, 'learning_rate': 4.12e-05, 'epoch': 0.18}\n",
      "{'loss': 0.3099, 'grad_norm': 0.9066352844238281, 'learning_rate': 4.1120000000000006e-05, 'epoch': 0.18}\n",
      "{'loss': 0.2406, 'grad_norm': 3.0444204807281494, 'learning_rate': 4.104e-05, 'epoch': 0.18}\n",
      "{'loss': 0.1775, 'grad_norm': 1.6477071046829224, 'learning_rate': 4.096e-05, 'epoch': 0.18}\n",
      "{'loss': 0.2423, 'grad_norm': 2.7106637954711914, 'learning_rate': 4.088e-05, 'epoch': 0.18}\n",
      "{'loss': 0.2438, 'grad_norm': 2.4866909980773926, 'learning_rate': 4.08e-05, 'epoch': 0.18}\n",
      "{'loss': 0.2509, 'grad_norm': 4.400035858154297, 'learning_rate': 4.072e-05, 'epoch': 0.19}\n",
      "{'loss': 0.3227, 'grad_norm': 1.6984957456588745, 'learning_rate': 4.064e-05, 'epoch': 0.19}\n",
      "{'loss': 0.2361, 'grad_norm': 3.980578660964966, 'learning_rate': 4.0560000000000005e-05, 'epoch': 0.19}\n",
      "{'loss': 0.2753, 'grad_norm': 2.5118062496185303, 'learning_rate': 4.048e-05, 'epoch': 0.19}\n",
      "{'loss': 0.2348, 'grad_norm': 1.9136161804199219, 'learning_rate': 4.0400000000000006e-05, 'epoch': 0.19}\n",
      "{'loss': 0.2177, 'grad_norm': 2.3572514057159424, 'learning_rate': 4.032e-05, 'epoch': 0.19}\n",
      "{'loss': 0.2195, 'grad_norm': 1.9518080949783325, 'learning_rate': 4.024e-05, 'epoch': 0.2}\n",
      "{'loss': 0.2928, 'grad_norm': 6.432658672332764, 'learning_rate': 4.016e-05, 'epoch': 0.2}\n",
      "{'loss': 0.206, 'grad_norm': 3.6063520908355713, 'learning_rate': 4.008e-05, 'epoch': 0.2}\n",
      "{'loss': 0.1428, 'grad_norm': 5.903524398803711, 'learning_rate': 4e-05, 'epoch': 0.2}\n",
      "{'loss': 0.34, 'grad_norm': 3.476729154586792, 'learning_rate': 3.9920000000000004e-05, 'epoch': 0.2}\n",
      "{'loss': 0.1887, 'grad_norm': 2.2883825302124023, 'learning_rate': 3.984e-05, 'epoch': 0.2}\n",
      "{'loss': 0.1886, 'grad_norm': 4.695982456207275, 'learning_rate': 3.9760000000000006e-05, 'epoch': 0.2}\n",
      "{'loss': 0.1599, 'grad_norm': 1.57406747341156, 'learning_rate': 3.968e-05, 'epoch': 0.21}\n",
      "{'loss': 0.2643, 'grad_norm': 1.6739534139633179, 'learning_rate': 3.960000000000001e-05, 'epoch': 0.21}\n",
      "{'loss': 0.2424, 'grad_norm': 0.7546713948249817, 'learning_rate': 3.952e-05, 'epoch': 0.21}\n",
      "{'loss': 0.307, 'grad_norm': 2.128446102142334, 'learning_rate': 3.944e-05, 'epoch': 0.21}\n",
      "{'loss': 0.4527, 'grad_norm': 2.7008073329925537, 'learning_rate': 3.936e-05, 'epoch': 0.21}\n",
      "{'loss': 0.2379, 'grad_norm': 3.43220853805542, 'learning_rate': 3.9280000000000003e-05, 'epoch': 0.21}\n",
      "{'loss': 0.2459, 'grad_norm': 2.054720163345337, 'learning_rate': 3.9200000000000004e-05, 'epoch': 0.22}\n",
      "{'loss': 0.2114, 'grad_norm': 2.6731820106506348, 'learning_rate': 3.912e-05, 'epoch': 0.22}\n",
      "{'loss': 0.2473, 'grad_norm': 3.5172600746154785, 'learning_rate': 3.9040000000000006e-05, 'epoch': 0.22}\n",
      "{'loss': 0.3343, 'grad_norm': 4.856107711791992, 'learning_rate': 3.896e-05, 'epoch': 0.22}\n",
      "{'loss': 0.2638, 'grad_norm': 1.9785088300704956, 'learning_rate': 3.888e-05, 'epoch': 0.22}\n",
      "{'loss': 0.2567, 'grad_norm': 3.4031739234924316, 'learning_rate': 3.88e-05, 'epoch': 0.22}\n",
      "{'loss': 0.352, 'grad_norm': 1.8409639596939087, 'learning_rate': 3.872e-05, 'epoch': 0.23}\n",
      "{'loss': 0.2351, 'grad_norm': 2.6900572776794434, 'learning_rate': 3.864e-05, 'epoch': 0.23}\n",
      "{'loss': 0.2486, 'grad_norm': 1.6763478517532349, 'learning_rate': 3.8560000000000004e-05, 'epoch': 0.23}\n",
      "{'loss': 0.2685, 'grad_norm': 3.698678493499756, 'learning_rate': 3.848e-05, 'epoch': 0.23}\n",
      "{'loss': 0.2475, 'grad_norm': 1.947597861289978, 'learning_rate': 3.8400000000000005e-05, 'epoch': 0.23}\n",
      "{'loss': 0.2741, 'grad_norm': 2.682077646255493, 'learning_rate': 3.832e-05, 'epoch': 0.23}\n",
      "{'loss': 0.2503, 'grad_norm': 1.3479474782943726, 'learning_rate': 3.8240000000000007e-05, 'epoch': 0.24}\n",
      "{'loss': 0.2973, 'grad_norm': 2.739863634109497, 'learning_rate': 3.816e-05, 'epoch': 0.24}\n",
      "{'loss': 0.2851, 'grad_norm': 5.045321464538574, 'learning_rate': 3.808e-05, 'epoch': 0.24}\n",
      "{'loss': 0.2571, 'grad_norm': 8.167257308959961, 'learning_rate': 3.8e-05, 'epoch': 0.24}\n",
      "{'loss': 0.1723, 'grad_norm': 1.0079208612442017, 'learning_rate': 3.792e-05, 'epoch': 0.24}\n",
      "{'loss': 0.211, 'grad_norm': 3.6639108657836914, 'learning_rate': 3.7840000000000004e-05, 'epoch': 0.24}\n",
      "{'loss': 0.2382, 'grad_norm': 2.6408092975616455, 'learning_rate': 3.776e-05, 'epoch': 0.24}\n",
      "{'loss': 0.1477, 'grad_norm': 2.1906790733337402, 'learning_rate': 3.7680000000000005e-05, 'epoch': 0.25}\n",
      "{'loss': 0.2454, 'grad_norm': 3.9967894554138184, 'learning_rate': 3.76e-05, 'epoch': 0.25}\n",
      "{'loss': 0.231, 'grad_norm': 4.571254253387451, 'learning_rate': 3.752e-05, 'epoch': 0.25}\n",
      "{'loss': 0.221, 'grad_norm': 2.335254192352295, 'learning_rate': 3.744e-05, 'epoch': 0.25}\n",
      "{'loss': 0.3051, 'grad_norm': 3.9812681674957275, 'learning_rate': 3.736e-05, 'epoch': 0.25}\n",
      "{'loss': 0.1933, 'grad_norm': 2.4525721073150635, 'learning_rate': 3.728e-05, 'epoch': 0.25}\n",
      "{'loss': 0.2685, 'grad_norm': 1.7919623851776123, 'learning_rate': 3.72e-05, 'epoch': 0.26}\n",
      "{'loss': 0.2325, 'grad_norm': 2.892514944076538, 'learning_rate': 3.712e-05, 'epoch': 0.26}\n",
      "{'loss': 0.2541, 'grad_norm': 1.3252649307250977, 'learning_rate': 3.7040000000000005e-05, 'epoch': 0.26}\n",
      "{'loss': 0.2667, 'grad_norm': 2.095075845718384, 'learning_rate': 3.696e-05, 'epoch': 0.26}\n",
      "{'loss': 0.2233, 'grad_norm': 4.45612096786499, 'learning_rate': 3.6880000000000006e-05, 'epoch': 0.26}\n",
      "{'loss': 0.2053, 'grad_norm': 3.032390594482422, 'learning_rate': 3.68e-05, 'epoch': 0.26}\n",
      "{'loss': 0.1913, 'grad_norm': 3.864441156387329, 'learning_rate': 3.672000000000001e-05, 'epoch': 0.27}\n",
      "{'loss': 0.2052, 'grad_norm': 6.562683582305908, 'learning_rate': 3.664e-05, 'epoch': 0.27}\n",
      "{'loss': 0.1837, 'grad_norm': 4.589919567108154, 'learning_rate': 3.656e-05, 'epoch': 0.27}\n",
      "{'loss': 0.3046, 'grad_norm': 3.6711323261260986, 'learning_rate': 3.648e-05, 'epoch': 0.27}\n",
      "{'loss': 0.2857, 'grad_norm': 4.543206214904785, 'learning_rate': 3.6400000000000004e-05, 'epoch': 0.27}\n",
      "{'loss': 0.3178, 'grad_norm': 3.5109705924987793, 'learning_rate': 3.6320000000000005e-05, 'epoch': 0.27}\n",
      "{'loss': 0.1596, 'grad_norm': 1.602471113204956, 'learning_rate': 3.624e-05, 'epoch': 0.28}\n",
      "{'loss': 0.1977, 'grad_norm': 3.975125312805176, 'learning_rate': 3.616e-05, 'epoch': 0.28}\n",
      "{'loss': 0.1698, 'grad_norm': 2.151587963104248, 'learning_rate': 3.608e-05, 'epoch': 0.28}\n",
      "{'loss': 0.1798, 'grad_norm': 1.5673613548278809, 'learning_rate': 3.6e-05, 'epoch': 0.28}\n",
      "{'loss': 0.2212, 'grad_norm': 3.900834321975708, 'learning_rate': 3.592e-05, 'epoch': 0.28}\n",
      "{'loss': 0.2113, 'grad_norm': 2.6693267822265625, 'learning_rate': 3.584e-05, 'epoch': 0.28}\n",
      "{'loss': 0.3239, 'grad_norm': 4.597334861755371, 'learning_rate': 3.5759999999999996e-05, 'epoch': 0.28}\n",
      "{'loss': 0.3153, 'grad_norm': 3.1402933597564697, 'learning_rate': 3.5680000000000004e-05, 'epoch': 0.29}\n",
      "{'loss': 0.2331, 'grad_norm': 1.9607744216918945, 'learning_rate': 3.56e-05, 'epoch': 0.29}\n",
      "{'loss': 0.3019, 'grad_norm': 2.662414789199829, 'learning_rate': 3.5520000000000006e-05, 'epoch': 0.29}\n",
      "{'loss': 0.2449, 'grad_norm': 0.9185004234313965, 'learning_rate': 3.544e-05, 'epoch': 0.29}\n",
      "{'loss': 0.2907, 'grad_norm': 3.3215396404266357, 'learning_rate': 3.536000000000001e-05, 'epoch': 0.29}\n",
      "{'loss': 0.2636, 'grad_norm': 4.060776710510254, 'learning_rate': 3.528e-05, 'epoch': 0.29}\n",
      "{'loss': 0.3428, 'grad_norm': 0.7567640542984009, 'learning_rate': 3.52e-05, 'epoch': 0.3}\n",
      "{'loss': 0.2506, 'grad_norm': 3.080780029296875, 'learning_rate': 3.512e-05, 'epoch': 0.3}\n",
      "{'loss': 0.2206, 'grad_norm': 1.347535490989685, 'learning_rate': 3.504e-05, 'epoch': 0.3}\n",
      "{'loss': 0.245, 'grad_norm': 2.789483070373535, 'learning_rate': 3.4960000000000004e-05, 'epoch': 0.3}\n",
      "{'loss': 0.2726, 'grad_norm': 2.480703353881836, 'learning_rate': 3.4880000000000005e-05, 'epoch': 0.3}\n",
      "{'loss': 0.3054, 'grad_norm': 1.506905198097229, 'learning_rate': 3.48e-05, 'epoch': 0.3}\n",
      "{'loss': 0.2208, 'grad_norm': 1.7177716493606567, 'learning_rate': 3.472e-05, 'epoch': 0.31}\n",
      "{'loss': 0.2099, 'grad_norm': 2.978541612625122, 'learning_rate': 3.464e-05, 'epoch': 0.31}\n",
      "{'loss': 0.2078, 'grad_norm': 1.7189606428146362, 'learning_rate': 3.456e-05, 'epoch': 0.31}\n",
      "{'loss': 0.2042, 'grad_norm': 1.9464302062988281, 'learning_rate': 3.448e-05, 'epoch': 0.31}\n",
      "{'loss': 0.2706, 'grad_norm': 4.449612140655518, 'learning_rate': 3.4399999999999996e-05, 'epoch': 0.31}\n",
      "{'loss': 0.3188, 'grad_norm': 2.396914005279541, 'learning_rate': 3.4320000000000003e-05, 'epoch': 0.31}\n",
      "{'loss': 0.2394, 'grad_norm': 2.825382947921753, 'learning_rate': 3.424e-05, 'epoch': 0.32}\n",
      "{'loss': 0.2127, 'grad_norm': 1.7290568351745605, 'learning_rate': 3.4160000000000005e-05, 'epoch': 0.32}\n",
      "{'loss': 0.2765, 'grad_norm': 0.8116180300712585, 'learning_rate': 3.408e-05, 'epoch': 0.32}\n",
      "{'loss': 0.1726, 'grad_norm': 1.0024698972702026, 'learning_rate': 3.4000000000000007e-05, 'epoch': 0.32}\n",
      "{'loss': 0.1703, 'grad_norm': 3.7158126831054688, 'learning_rate': 3.392e-05, 'epoch': 0.32}\n",
      "{'loss': 0.2538, 'grad_norm': 1.3214527368545532, 'learning_rate': 3.384e-05, 'epoch': 0.32}\n",
      "{'loss': 0.1801, 'grad_norm': 3.04707407951355, 'learning_rate': 3.376e-05, 'epoch': 0.32}\n",
      "{'loss': 0.2458, 'grad_norm': 5.38111686706543, 'learning_rate': 3.368e-05, 'epoch': 0.33}\n",
      "{'loss': 0.3048, 'grad_norm': 1.6486332416534424, 'learning_rate': 3.3600000000000004e-05, 'epoch': 0.33}\n",
      "{'loss': 0.3293, 'grad_norm': 3.9657323360443115, 'learning_rate': 3.3520000000000004e-05, 'epoch': 0.33}\n",
      "{'loss': 0.1144, 'grad_norm': 0.489879846572876, 'learning_rate': 3.344e-05, 'epoch': 0.33}\n",
      "{'loss': 0.2493, 'grad_norm': 2.6796815395355225, 'learning_rate': 3.336e-05, 'epoch': 0.33}\n",
      "{'loss': 0.1584, 'grad_norm': 0.6058939099311829, 'learning_rate': 3.328e-05, 'epoch': 0.33}\n",
      "{'loss': 0.2337, 'grad_norm': 2.757159471511841, 'learning_rate': 3.32e-05, 'epoch': 0.34}\n",
      "{'loss': 0.2612, 'grad_norm': 4.946295261383057, 'learning_rate': 3.312e-05, 'epoch': 0.34}\n",
      "{'loss': 0.3336, 'grad_norm': 4.533501148223877, 'learning_rate': 3.304e-05, 'epoch': 0.34}\n",
      "{'loss': 0.1642, 'grad_norm': 1.9766253232955933, 'learning_rate': 3.296e-05, 'epoch': 0.34}\n",
      "{'loss': 0.2233, 'grad_norm': 2.2285120487213135, 'learning_rate': 3.288e-05, 'epoch': 0.34}\n",
      "{'loss': 0.2459, 'grad_norm': 0.745198667049408, 'learning_rate': 3.2800000000000004e-05, 'epoch': 0.34}\n",
      "{'loss': 0.2721, 'grad_norm': 3.5479564666748047, 'learning_rate': 3.272e-05, 'epoch': 0.35}\n",
      "{'loss': 0.3103, 'grad_norm': 5.309443473815918, 'learning_rate': 3.2640000000000006e-05, 'epoch': 0.35}\n",
      "{'loss': 0.2261, 'grad_norm': 1.8466260433197021, 'learning_rate': 3.256e-05, 'epoch': 0.35}\n",
      "{'loss': 0.3298, 'grad_norm': 2.8486287593841553, 'learning_rate': 3.248e-05, 'epoch': 0.35}\n",
      "{'loss': 0.2807, 'grad_norm': 2.8010926246643066, 'learning_rate': 3.24e-05, 'epoch': 0.35}\n",
      "{'loss': 0.1958, 'grad_norm': 0.8255437016487122, 'learning_rate': 3.232e-05, 'epoch': 0.35}\n",
      "{'loss': 0.1899, 'grad_norm': 1.1727341413497925, 'learning_rate': 3.224e-05, 'epoch': 0.36}\n",
      "{'loss': 0.2514, 'grad_norm': 3.1342592239379883, 'learning_rate': 3.2160000000000004e-05, 'epoch': 0.36}\n",
      "{'loss': 0.1969, 'grad_norm': 2.014813184738159, 'learning_rate': 3.208e-05, 'epoch': 0.36}\n",
      "{'loss': 0.2217, 'grad_norm': 3.059021472930908, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.36}\n",
      "{'loss': 0.1791, 'grad_norm': 1.7777807712554932, 'learning_rate': 3.192e-05, 'epoch': 0.36}\n",
      "{'loss': 0.2666, 'grad_norm': 3.2929418087005615, 'learning_rate': 3.184e-05, 'epoch': 0.36}\n",
      "{'loss': 0.2318, 'grad_norm': 2.271495819091797, 'learning_rate': 3.176e-05, 'epoch': 0.36}\n",
      "{'loss': 0.1872, 'grad_norm': 1.642846941947937, 'learning_rate': 3.168e-05, 'epoch': 0.37}\n",
      "{'loss': 0.1183, 'grad_norm': 3.219761610031128, 'learning_rate': 3.16e-05, 'epoch': 0.37}\n",
      "{'loss': 0.2472, 'grad_norm': 5.330394744873047, 'learning_rate': 3.1519999999999996e-05, 'epoch': 0.37}\n",
      "{'loss': 0.1974, 'grad_norm': 2.7638392448425293, 'learning_rate': 3.1440000000000004e-05, 'epoch': 0.37}\n",
      "{'loss': 0.3225, 'grad_norm': 3.849818706512451, 'learning_rate': 3.136e-05, 'epoch': 0.37}\n",
      "{'loss': 0.3847, 'grad_norm': 4.066520690917969, 'learning_rate': 3.1280000000000005e-05, 'epoch': 0.37}\n",
      "{'loss': 0.2746, 'grad_norm': 6.556169509887695, 'learning_rate': 3.12e-05, 'epoch': 0.38}\n",
      "{'loss': 0.1587, 'grad_norm': 1.4846099615097046, 'learning_rate': 3.112e-05, 'epoch': 0.38}\n",
      "{'loss': 0.2383, 'grad_norm': 1.8721957206726074, 'learning_rate': 3.104e-05, 'epoch': 0.38}\n",
      "{'loss': 0.2895, 'grad_norm': 2.6667442321777344, 'learning_rate': 3.096e-05, 'epoch': 0.38}\n",
      "{'loss': 0.1481, 'grad_norm': 0.9667590260505676, 'learning_rate': 3.088e-05, 'epoch': 0.38}\n",
      "{'loss': 0.204, 'grad_norm': 2.3379132747650146, 'learning_rate': 3.08e-05, 'epoch': 0.38}\n",
      "{'loss': 0.2379, 'grad_norm': 3.8161308765411377, 'learning_rate': 3.072e-05, 'epoch': 0.39}\n",
      "{'loss': 0.2388, 'grad_norm': 2.492088794708252, 'learning_rate': 3.0640000000000005e-05, 'epoch': 0.39}\n",
      "{'loss': 0.2476, 'grad_norm': 4.77614164352417, 'learning_rate': 3.056e-05, 'epoch': 0.39}\n",
      "{'loss': 0.2072, 'grad_norm': 3.196093797683716, 'learning_rate': 3.0480000000000003e-05, 'epoch': 0.39}\n",
      "{'loss': 0.2329, 'grad_norm': 1.4774481058120728, 'learning_rate': 3.04e-05, 'epoch': 0.39}\n",
      "{'loss': 0.1876, 'grad_norm': 1.6348941326141357, 'learning_rate': 3.0320000000000004e-05, 'epoch': 0.39}\n",
      "{'loss': 0.1573, 'grad_norm': 2.501033306121826, 'learning_rate': 3.0240000000000002e-05, 'epoch': 0.4}\n",
      "{'loss': 0.1832, 'grad_norm': 2.6571388244628906, 'learning_rate': 3.016e-05, 'epoch': 0.4}\n",
      "{'loss': 0.2892, 'grad_norm': 2.463200330734253, 'learning_rate': 3.0080000000000003e-05, 'epoch': 0.4}\n",
      "{'loss': 0.2752, 'grad_norm': 3.8695552349090576, 'learning_rate': 3e-05, 'epoch': 0.4}\n",
      "{'loss': 0.1842, 'grad_norm': 4.961655139923096, 'learning_rate': 2.9920000000000005e-05, 'epoch': 0.4}\n",
      "{'loss': 0.3585, 'grad_norm': 3.6606624126434326, 'learning_rate': 2.9840000000000002e-05, 'epoch': 0.4}\n",
      "{'loss': 0.2635, 'grad_norm': 2.891190528869629, 'learning_rate': 2.976e-05, 'epoch': 0.4}\n",
      "{'loss': 0.2105, 'grad_norm': 1.6648061275482178, 'learning_rate': 2.9680000000000004e-05, 'epoch': 0.41}\n",
      "{'loss': 0.183, 'grad_norm': 1.6827267408370972, 'learning_rate': 2.96e-05, 'epoch': 0.41}\n",
      "{'loss': 0.2538, 'grad_norm': 2.7943918704986572, 'learning_rate': 2.9520000000000002e-05, 'epoch': 0.41}\n",
      "{'loss': 0.3227, 'grad_norm': 2.3814804553985596, 'learning_rate': 2.944e-05, 'epoch': 0.41}\n",
      "{'loss': 0.1829, 'grad_norm': 2.012697458267212, 'learning_rate': 2.9360000000000003e-05, 'epoch': 0.41}\n",
      "{'loss': 0.2602, 'grad_norm': 6.099246978759766, 'learning_rate': 2.928e-05, 'epoch': 0.41}\n",
      "{'loss': 0.2545, 'grad_norm': 4.352447986602783, 'learning_rate': 2.9199999999999998e-05, 'epoch': 0.42}\n",
      "{'loss': 0.2598, 'grad_norm': 1.6587719917297363, 'learning_rate': 2.9120000000000002e-05, 'epoch': 0.42}\n",
      "{'loss': 0.2024, 'grad_norm': 1.6154474020004272, 'learning_rate': 2.904e-05, 'epoch': 0.42}\n",
      "{'loss': 0.2233, 'grad_norm': 2.183579683303833, 'learning_rate': 2.8960000000000004e-05, 'epoch': 0.42}\n",
      "{'loss': 0.3021, 'grad_norm': 2.321601629257202, 'learning_rate': 2.888e-05, 'epoch': 0.42}\n",
      "{'loss': 0.2194, 'grad_norm': 1.8325154781341553, 'learning_rate': 2.88e-05, 'epoch': 0.42}\n",
      "{'loss': 0.1999, 'grad_norm': 2.9826347827911377, 'learning_rate': 2.8720000000000003e-05, 'epoch': 0.43}\n",
      "{'loss': 0.1996, 'grad_norm': 5.500946998596191, 'learning_rate': 2.864e-05, 'epoch': 0.43}\n",
      "{'loss': 0.2293, 'grad_norm': 1.183903455734253, 'learning_rate': 2.8560000000000004e-05, 'epoch': 0.43}\n",
      "{'loss': 0.2377, 'grad_norm': 1.0246719121932983, 'learning_rate': 2.8480000000000002e-05, 'epoch': 0.43}\n",
      "{'loss': 0.2213, 'grad_norm': 3.1103153228759766, 'learning_rate': 2.84e-05, 'epoch': 0.43}\n",
      "{'loss': 0.2488, 'grad_norm': 1.0324219465255737, 'learning_rate': 2.8320000000000003e-05, 'epoch': 0.43}\n",
      "{'loss': 0.3481, 'grad_norm': 2.507185220718384, 'learning_rate': 2.824e-05, 'epoch': 0.44}\n",
      "{'loss': 0.2825, 'grad_norm': 2.486156463623047, 'learning_rate': 2.816e-05, 'epoch': 0.44}\n",
      "{'loss': 0.2923, 'grad_norm': 1.6243244409561157, 'learning_rate': 2.8080000000000002e-05, 'epoch': 0.44}\n",
      "{'loss': 0.219, 'grad_norm': 1.9712791442871094, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.44}\n",
      "{'loss': 0.2537, 'grad_norm': 2.3337979316711426, 'learning_rate': 2.792e-05, 'epoch': 0.44}\n",
      "{'loss': 0.2214, 'grad_norm': 2.531555414199829, 'learning_rate': 2.7839999999999998e-05, 'epoch': 0.44}\n",
      "{'loss': 0.1952, 'grad_norm': 5.202394962310791, 'learning_rate': 2.7760000000000002e-05, 'epoch': 0.44}\n",
      "{'loss': 0.1527, 'grad_norm': 2.664910078048706, 'learning_rate': 2.768e-05, 'epoch': 0.45}\n",
      "{'loss': 0.2521, 'grad_norm': 1.2503714561462402, 'learning_rate': 2.7600000000000003e-05, 'epoch': 0.45}\n",
      "{'loss': 0.1424, 'grad_norm': 0.7154605984687805, 'learning_rate': 2.752e-05, 'epoch': 0.45}\n",
      "{'loss': 0.205, 'grad_norm': 1.7952053546905518, 'learning_rate': 2.7439999999999998e-05, 'epoch': 0.45}\n",
      "{'loss': 0.229, 'grad_norm': 4.7187628746032715, 'learning_rate': 2.7360000000000002e-05, 'epoch': 0.45}\n",
      "{'loss': 0.2612, 'grad_norm': 3.995115041732788, 'learning_rate': 2.728e-05, 'epoch': 0.45}\n",
      "{'loss': 0.256, 'grad_norm': 3.752840042114258, 'learning_rate': 2.7200000000000004e-05, 'epoch': 0.46}\n",
      "{'loss': 0.1691, 'grad_norm': 5.2178473472595215, 'learning_rate': 2.712e-05, 'epoch': 0.46}\n",
      "{'loss': 0.2194, 'grad_norm': 2.6433277130126953, 'learning_rate': 2.704e-05, 'epoch': 0.46}\n",
      "{'loss': 0.2182, 'grad_norm': 0.6304879784584045, 'learning_rate': 2.6960000000000003e-05, 'epoch': 0.46}\n",
      "{'loss': 0.2933, 'grad_norm': 3.3426029682159424, 'learning_rate': 2.688e-05, 'epoch': 0.46}\n",
      "{'loss': 0.2705, 'grad_norm': 3.927320718765259, 'learning_rate': 2.6800000000000004e-05, 'epoch': 0.46}\n",
      "{'loss': 0.2458, 'grad_norm': 4.208600044250488, 'learning_rate': 2.672e-05, 'epoch': 0.47}\n",
      "{'loss': 0.2336, 'grad_norm': 4.033786296844482, 'learning_rate': 2.6640000000000002e-05, 'epoch': 0.47}\n",
      "{'loss': 0.1931, 'grad_norm': 4.062202453613281, 'learning_rate': 2.6560000000000003e-05, 'epoch': 0.47}\n",
      "{'loss': 0.2482, 'grad_norm': 3.2000818252563477, 'learning_rate': 2.648e-05, 'epoch': 0.47}\n",
      "{'loss': 0.3539, 'grad_norm': 2.3873965740203857, 'learning_rate': 2.64e-05, 'epoch': 0.47}\n",
      "{'loss': 0.2603, 'grad_norm': 2.733152389526367, 'learning_rate': 2.632e-05, 'epoch': 0.47}\n",
      "{'loss': 0.1706, 'grad_norm': 2.4634554386138916, 'learning_rate': 2.6240000000000003e-05, 'epoch': 0.48}\n",
      "{'loss': 0.2044, 'grad_norm': 0.9721844792366028, 'learning_rate': 2.616e-05, 'epoch': 0.48}\n",
      "{'loss': 0.2354, 'grad_norm': 1.174820065498352, 'learning_rate': 2.6079999999999998e-05, 'epoch': 0.48}\n",
      "{'loss': 0.1754, 'grad_norm': 2.1437559127807617, 'learning_rate': 2.6000000000000002e-05, 'epoch': 0.48}\n",
      "{'loss': 0.3501, 'grad_norm': 3.1522252559661865, 'learning_rate': 2.592e-05, 'epoch': 0.48}\n",
      "{'loss': 0.2234, 'grad_norm': 3.380385160446167, 'learning_rate': 2.5840000000000003e-05, 'epoch': 0.48}\n",
      "{'loss': 0.351, 'grad_norm': 2.48112416267395, 'learning_rate': 2.576e-05, 'epoch': 0.48}\n",
      "{'loss': 0.2551, 'grad_norm': 5.361678600311279, 'learning_rate': 2.5679999999999998e-05, 'epoch': 0.49}\n",
      "{'loss': 0.2582, 'grad_norm': 3.8496527671813965, 'learning_rate': 2.5600000000000002e-05, 'epoch': 0.49}\n",
      "{'loss': 0.2204, 'grad_norm': 2.714951515197754, 'learning_rate': 2.552e-05, 'epoch': 0.49}\n",
      "{'loss': 0.2393, 'grad_norm': 3.6240718364715576, 'learning_rate': 2.5440000000000004e-05, 'epoch': 0.49}\n",
      "{'loss': 0.3033, 'grad_norm': 3.4079978466033936, 'learning_rate': 2.536e-05, 'epoch': 0.49}\n",
      "{'loss': 0.1872, 'grad_norm': 3.2786457538604736, 'learning_rate': 2.5280000000000005e-05, 'epoch': 0.49}\n",
      "{'loss': 0.2286, 'grad_norm': 3.1089606285095215, 'learning_rate': 2.5200000000000003e-05, 'epoch': 0.5}\n",
      "{'loss': 0.2458, 'grad_norm': 2.7475168704986572, 'learning_rate': 2.512e-05, 'epoch': 0.5}\n",
      "{'loss': 0.2701, 'grad_norm': 3.176280975341797, 'learning_rate': 2.504e-05, 'epoch': 0.5}\n",
      "{'loss': 0.199, 'grad_norm': 1.7141797542572021, 'learning_rate': 2.496e-05, 'epoch': 0.5}\n",
      "{'loss': 0.1536, 'grad_norm': 2.0468642711639404, 'learning_rate': 2.488e-05, 'epoch': 0.5}\n",
      "{'loss': 0.3053, 'grad_norm': 4.363279342651367, 'learning_rate': 2.48e-05, 'epoch': 0.5}\n",
      "{'loss': 0.1999, 'grad_norm': 1.2754813432693481, 'learning_rate': 2.472e-05, 'epoch': 0.51}\n",
      "{'loss': 0.252, 'grad_norm': 0.7204574346542358, 'learning_rate': 2.464e-05, 'epoch': 0.51}\n",
      "{'loss': 0.1972, 'grad_norm': 1.6575673818588257, 'learning_rate': 2.4560000000000002e-05, 'epoch': 0.51}\n",
      "{'loss': 0.3074, 'grad_norm': 2.4302780628204346, 'learning_rate': 2.448e-05, 'epoch': 0.51}\n",
      "{'loss': 0.1842, 'grad_norm': 0.4848043620586395, 'learning_rate': 2.44e-05, 'epoch': 0.51}\n",
      "{'loss': 0.2667, 'grad_norm': 1.7121721506118774, 'learning_rate': 2.432e-05, 'epoch': 0.51}\n",
      "{'loss': 0.3121, 'grad_norm': 2.0093722343444824, 'learning_rate': 2.4240000000000002e-05, 'epoch': 0.52}\n",
      "{'loss': 0.2394, 'grad_norm': 2.8737189769744873, 'learning_rate': 2.4160000000000002e-05, 'epoch': 0.52}\n",
      "{'loss': 0.228, 'grad_norm': 4.3297600746154785, 'learning_rate': 2.408e-05, 'epoch': 0.52}\n",
      "{'loss': 0.3105, 'grad_norm': 2.8728973865509033, 'learning_rate': 2.4e-05, 'epoch': 0.52}\n",
      "{'loss': 0.3774, 'grad_norm': 3.6132490634918213, 'learning_rate': 2.392e-05, 'epoch': 0.52}\n",
      "{'loss': 0.2345, 'grad_norm': 3.6523547172546387, 'learning_rate': 2.3840000000000002e-05, 'epoch': 0.52}\n",
      "{'loss': 0.277, 'grad_norm': 2.43825626373291, 'learning_rate': 2.3760000000000003e-05, 'epoch': 0.52}\n",
      "{'loss': 0.2315, 'grad_norm': 1.8542604446411133, 'learning_rate': 2.3680000000000004e-05, 'epoch': 0.53}\n",
      "{'loss': 0.2687, 'grad_norm': 2.5542781352996826, 'learning_rate': 2.36e-05, 'epoch': 0.53}\n",
      "{'loss': 0.2473, 'grad_norm': 5.663676738739014, 'learning_rate': 2.3520000000000002e-05, 'epoch': 0.53}\n",
      "{'loss': 0.173, 'grad_norm': 2.41986346244812, 'learning_rate': 2.344e-05, 'epoch': 0.53}\n",
      "{'loss': 0.1977, 'grad_norm': 1.7724847793579102, 'learning_rate': 2.336e-05, 'epoch': 0.53}\n",
      "{'loss': 0.237, 'grad_norm': 3.8580193519592285, 'learning_rate': 2.328e-05, 'epoch': 0.53}\n",
      "{'loss': 0.2261, 'grad_norm': 2.2019240856170654, 'learning_rate': 2.32e-05, 'epoch': 0.54}\n",
      "{'loss': 0.1564, 'grad_norm': 1.3229358196258545, 'learning_rate': 2.312e-05, 'epoch': 0.54}\n",
      "{'loss': 0.2587, 'grad_norm': 6.05772590637207, 'learning_rate': 2.304e-05, 'epoch': 0.54}\n",
      "{'loss': 0.2849, 'grad_norm': 6.6941962242126465, 'learning_rate': 2.296e-05, 'epoch': 0.54}\n",
      "{'loss': 0.2269, 'grad_norm': 1.9816950559616089, 'learning_rate': 2.288e-05, 'epoch': 0.54}\n",
      "{'loss': 0.266, 'grad_norm': 5.185435771942139, 'learning_rate': 2.2800000000000002e-05, 'epoch': 0.54}\n",
      "{'loss': 0.2441, 'grad_norm': 2.5067427158355713, 'learning_rate': 2.2720000000000003e-05, 'epoch': 0.55}\n",
      "{'loss': 0.2196, 'grad_norm': 2.685849666595459, 'learning_rate': 2.264e-05, 'epoch': 0.55}\n",
      "{'loss': 0.2504, 'grad_norm': 1.9762276411056519, 'learning_rate': 2.256e-05, 'epoch': 0.55}\n",
      "{'loss': 0.1158, 'grad_norm': 1.2143396139144897, 'learning_rate': 2.248e-05, 'epoch': 0.55}\n",
      "{'loss': 0.2306, 'grad_norm': 3.719270944595337, 'learning_rate': 2.2400000000000002e-05, 'epoch': 0.55}\n",
      "{'loss': 0.2756, 'grad_norm': 5.43961238861084, 'learning_rate': 2.2320000000000003e-05, 'epoch': 0.55}\n",
      "{'loss': 0.1998, 'grad_norm': 3.614992618560791, 'learning_rate': 2.224e-05, 'epoch': 0.56}\n",
      "{'loss': 0.2398, 'grad_norm': 4.078737735748291, 'learning_rate': 2.216e-05, 'epoch': 0.56}\n",
      "{'loss': 0.1606, 'grad_norm': 2.137345314025879, 'learning_rate': 2.2080000000000002e-05, 'epoch': 0.56}\n",
      "{'loss': 0.2206, 'grad_norm': 3.9173173904418945, 'learning_rate': 2.2000000000000003e-05, 'epoch': 0.56}\n",
      "{'loss': 0.2425, 'grad_norm': 2.9694900512695312, 'learning_rate': 2.192e-05, 'epoch': 0.56}\n",
      "{'loss': 0.2572, 'grad_norm': 2.8914682865142822, 'learning_rate': 2.184e-05, 'epoch': 0.56}\n",
      "{'loss': 0.2865, 'grad_norm': 4.87455940246582, 'learning_rate': 2.176e-05, 'epoch': 0.56}\n",
      "{'loss': 0.1895, 'grad_norm': 4.061917781829834, 'learning_rate': 2.168e-05, 'epoch': 0.57}\n",
      "{'loss': 0.258, 'grad_norm': 2.4195504188537598, 'learning_rate': 2.16e-05, 'epoch': 0.57}\n",
      "{'loss': 0.2555, 'grad_norm': 2.40387225151062, 'learning_rate': 2.152e-05, 'epoch': 0.57}\n",
      "{'loss': 0.1585, 'grad_norm': 2.010434627532959, 'learning_rate': 2.144e-05, 'epoch': 0.57}\n",
      "{'loss': 0.2896, 'grad_norm': 3.559865951538086, 'learning_rate': 2.1360000000000002e-05, 'epoch': 0.57}\n",
      "{'loss': 0.2992, 'grad_norm': 3.0023651123046875, 'learning_rate': 2.128e-05, 'epoch': 0.57}\n",
      "{'loss': 0.1411, 'grad_norm': 2.30639910697937, 'learning_rate': 2.12e-05, 'epoch': 0.58}\n",
      "{'loss': 0.2682, 'grad_norm': 1.9279779195785522, 'learning_rate': 2.112e-05, 'epoch': 0.58}\n",
      "{'loss': 0.1437, 'grad_norm': 1.6746985912322998, 'learning_rate': 2.1040000000000002e-05, 'epoch': 0.58}\n",
      "{'loss': 0.2188, 'grad_norm': 3.6211304664611816, 'learning_rate': 2.0960000000000003e-05, 'epoch': 0.58}\n",
      "{'loss': 0.2638, 'grad_norm': 3.307474136352539, 'learning_rate': 2.0880000000000003e-05, 'epoch': 0.58}\n",
      "{'loss': 0.1978, 'grad_norm': 2.8096165657043457, 'learning_rate': 2.08e-05, 'epoch': 0.58}\n",
      "{'loss': 0.1688, 'grad_norm': 3.887908935546875, 'learning_rate': 2.072e-05, 'epoch': 0.59}\n",
      "{'loss': 0.2739, 'grad_norm': 1.2957733869552612, 'learning_rate': 2.0640000000000002e-05, 'epoch': 0.59}\n",
      "{'loss': 0.3068, 'grad_norm': 3.326488733291626, 'learning_rate': 2.0560000000000003e-05, 'epoch': 0.59}\n",
      "{'loss': 0.2823, 'grad_norm': 3.344186544418335, 'learning_rate': 2.048e-05, 'epoch': 0.59}\n",
      "{'loss': 0.2316, 'grad_norm': 3.8042330741882324, 'learning_rate': 2.04e-05, 'epoch': 0.59}\n",
      "{'loss': 0.1689, 'grad_norm': 2.609616756439209, 'learning_rate': 2.032e-05, 'epoch': 0.59}\n",
      "{'loss': 0.1573, 'grad_norm': 1.0428024530410767, 'learning_rate': 2.024e-05, 'epoch': 0.6}\n",
      "{'loss': 0.2255, 'grad_norm': 2.332674026489258, 'learning_rate': 2.016e-05, 'epoch': 0.6}\n",
      "{'loss': 0.2519, 'grad_norm': 2.0424251556396484, 'learning_rate': 2.008e-05, 'epoch': 0.6}\n",
      "{'loss': 0.1929, 'grad_norm': 4.169882774353027, 'learning_rate': 2e-05, 'epoch': 0.6}\n",
      "{'loss': 0.214, 'grad_norm': 2.6579251289367676, 'learning_rate': 1.992e-05, 'epoch': 0.6}\n",
      "{'loss': 0.2011, 'grad_norm': 1.2971163988113403, 'learning_rate': 1.984e-05, 'epoch': 0.6}\n",
      "{'loss': 0.2148, 'grad_norm': 1.7960126399993896, 'learning_rate': 1.976e-05, 'epoch': 0.6}\n",
      "{'loss': 0.2045, 'grad_norm': 2.755537509918213, 'learning_rate': 1.968e-05, 'epoch': 0.61}\n",
      "{'loss': 0.2433, 'grad_norm': 7.486672401428223, 'learning_rate': 1.9600000000000002e-05, 'epoch': 0.61}\n",
      "{'loss': 0.2689, 'grad_norm': 3.3495025634765625, 'learning_rate': 1.9520000000000003e-05, 'epoch': 0.61}\n",
      "{'loss': 0.1783, 'grad_norm': 0.3476450741291046, 'learning_rate': 1.944e-05, 'epoch': 0.61}\n",
      "{'loss': 0.2106, 'grad_norm': 2.3309645652770996, 'learning_rate': 1.936e-05, 'epoch': 0.61}\n",
      "{'loss': 0.2562, 'grad_norm': 3.8408637046813965, 'learning_rate': 1.9280000000000002e-05, 'epoch': 0.61}\n",
      "{'loss': 0.1688, 'grad_norm': 2.6656923294067383, 'learning_rate': 1.9200000000000003e-05, 'epoch': 0.62}\n",
      "{'loss': 0.2531, 'grad_norm': 2.711480140686035, 'learning_rate': 1.9120000000000003e-05, 'epoch': 0.62}\n",
      "{'loss': 0.1712, 'grad_norm': 1.5778530836105347, 'learning_rate': 1.904e-05, 'epoch': 0.62}\n",
      "{'loss': 0.2391, 'grad_norm': 2.49467396736145, 'learning_rate': 1.896e-05, 'epoch': 0.62}\n",
      "{'loss': 0.2927, 'grad_norm': 2.2900545597076416, 'learning_rate': 1.888e-05, 'epoch': 0.62}\n",
      "{'loss': 0.222, 'grad_norm': 0.6924616098403931, 'learning_rate': 1.88e-05, 'epoch': 0.62}\n",
      "{'loss': 0.2464, 'grad_norm': 4.964444160461426, 'learning_rate': 1.872e-05, 'epoch': 0.63}\n",
      "{'loss': 0.2694, 'grad_norm': 2.4508726596832275, 'learning_rate': 1.864e-05, 'epoch': 0.63}\n",
      "{'loss': 0.1992, 'grad_norm': 3.443488121032715, 'learning_rate': 1.856e-05, 'epoch': 0.63}\n",
      "{'loss': 0.2205, 'grad_norm': 2.482840061187744, 'learning_rate': 1.848e-05, 'epoch': 0.63}\n",
      "{'loss': 0.2099, 'grad_norm': 4.005091190338135, 'learning_rate': 1.84e-05, 'epoch': 0.63}\n",
      "{'loss': 0.1904, 'grad_norm': 2.375833749771118, 'learning_rate': 1.832e-05, 'epoch': 0.63}\n",
      "{'loss': 0.1542, 'grad_norm': 2.002166271209717, 'learning_rate': 1.824e-05, 'epoch': 0.64}\n",
      "{'loss': 0.249, 'grad_norm': 3.935178279876709, 'learning_rate': 1.8160000000000002e-05, 'epoch': 0.64}\n",
      "{'loss': 0.2174, 'grad_norm': 2.1869118213653564, 'learning_rate': 1.808e-05, 'epoch': 0.64}\n",
      "{'loss': 0.2747, 'grad_norm': 7.152611255645752, 'learning_rate': 1.8e-05, 'epoch': 0.64}\n",
      "{'loss': 0.2673, 'grad_norm': 0.4945511221885681, 'learning_rate': 1.792e-05, 'epoch': 0.64}\n",
      "{'loss': 0.3385, 'grad_norm': 3.0288643836975098, 'learning_rate': 1.7840000000000002e-05, 'epoch': 0.64}\n",
      "{'loss': 0.2154, 'grad_norm': 2.7138218879699707, 'learning_rate': 1.7760000000000003e-05, 'epoch': 0.64}\n",
      "{'loss': 0.3263, 'grad_norm': 1.8995471000671387, 'learning_rate': 1.7680000000000004e-05, 'epoch': 0.65}\n",
      "{'loss': 0.2898, 'grad_norm': 3.2061169147491455, 'learning_rate': 1.76e-05, 'epoch': 0.65}\n",
      "{'loss': 0.2672, 'grad_norm': 1.4217673540115356, 'learning_rate': 1.752e-05, 'epoch': 0.65}\n",
      "{'loss': 0.1536, 'grad_norm': 0.555859386920929, 'learning_rate': 1.7440000000000002e-05, 'epoch': 0.65}\n",
      "{'loss': 0.2208, 'grad_norm': 1.455276370048523, 'learning_rate': 1.736e-05, 'epoch': 0.65}\n",
      "{'loss': 0.2207, 'grad_norm': 3.367784023284912, 'learning_rate': 1.728e-05, 'epoch': 0.65}\n",
      "{'loss': 0.2339, 'grad_norm': 2.25630521774292, 'learning_rate': 1.7199999999999998e-05, 'epoch': 0.66}\n",
      "{'loss': 0.1896, 'grad_norm': 2.2714080810546875, 'learning_rate': 1.712e-05, 'epoch': 0.66}\n",
      "{'loss': 0.1955, 'grad_norm': 2.249802589416504, 'learning_rate': 1.704e-05, 'epoch': 0.66}\n",
      "{'loss': 0.1642, 'grad_norm': 0.41861796379089355, 'learning_rate': 1.696e-05, 'epoch': 0.66}\n",
      "{'loss': 0.1998, 'grad_norm': 3.932114601135254, 'learning_rate': 1.688e-05, 'epoch': 0.66}\n",
      "{'loss': 0.2592, 'grad_norm': 0.9225926995277405, 'learning_rate': 1.6800000000000002e-05, 'epoch': 0.66}\n",
      "{'loss': 0.2404, 'grad_norm': 3.377908945083618, 'learning_rate': 1.672e-05, 'epoch': 0.67}\n",
      "{'loss': 0.2424, 'grad_norm': 2.6404850482940674, 'learning_rate': 1.664e-05, 'epoch': 0.67}\n",
      "{'loss': 0.1791, 'grad_norm': 0.8611868619918823, 'learning_rate': 1.656e-05, 'epoch': 0.67}\n",
      "{'loss': 0.2214, 'grad_norm': 2.712308168411255, 'learning_rate': 1.648e-05, 'epoch': 0.67}\n",
      "{'loss': 0.1312, 'grad_norm': 2.197589635848999, 'learning_rate': 1.6400000000000002e-05, 'epoch': 0.67}\n",
      "{'loss': 0.216, 'grad_norm': 3.105679750442505, 'learning_rate': 1.6320000000000003e-05, 'epoch': 0.67}\n",
      "{'loss': 0.2911, 'grad_norm': 1.9443161487579346, 'learning_rate': 1.624e-05, 'epoch': 0.68}\n",
      "{'loss': 0.2114, 'grad_norm': 3.4292752742767334, 'learning_rate': 1.616e-05, 'epoch': 0.68}\n",
      "{'loss': 0.1855, 'grad_norm': 2.40771746635437, 'learning_rate': 1.6080000000000002e-05, 'epoch': 0.68}\n",
      "{'loss': 0.3107, 'grad_norm': 4.658906936645508, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.68}\n",
      "{'loss': 0.1195, 'grad_norm': 2.8850412368774414, 'learning_rate': 1.592e-05, 'epoch': 0.68}\n",
      "{'loss': 0.2426, 'grad_norm': 2.1728367805480957, 'learning_rate': 1.584e-05, 'epoch': 0.68}\n",
      "{'loss': 0.2152, 'grad_norm': 2.7962241172790527, 'learning_rate': 1.5759999999999998e-05, 'epoch': 0.68}\n",
      "{'loss': 0.1979, 'grad_norm': 1.950284719467163, 'learning_rate': 1.568e-05, 'epoch': 0.69}\n",
      "{'loss': 0.256, 'grad_norm': 2.528578758239746, 'learning_rate': 1.56e-05, 'epoch': 0.69}\n",
      "{'loss': 0.2182, 'grad_norm': 3.7645070552825928, 'learning_rate': 1.552e-05, 'epoch': 0.69}\n",
      "{'loss': 0.1855, 'grad_norm': 2.330829381942749, 'learning_rate': 1.544e-05, 'epoch': 0.69}\n",
      "{'loss': 0.2669, 'grad_norm': 2.4362919330596924, 'learning_rate': 1.536e-05, 'epoch': 0.69}\n",
      "{'loss': 0.2208, 'grad_norm': 2.641465425491333, 'learning_rate': 1.528e-05, 'epoch': 0.69}\n",
      "{'loss': 0.1297, 'grad_norm': 1.1339491605758667, 'learning_rate': 1.52e-05, 'epoch': 0.7}\n",
      "{'loss': 0.3374, 'grad_norm': 4.575369834899902, 'learning_rate': 1.5120000000000001e-05, 'epoch': 0.7}\n",
      "{'loss': 0.1992, 'grad_norm': 4.490025043487549, 'learning_rate': 1.5040000000000002e-05, 'epoch': 0.7}\n",
      "{'loss': 0.3056, 'grad_norm': 2.854890823364258, 'learning_rate': 1.4960000000000002e-05, 'epoch': 0.7}\n",
      "{'loss': 0.1519, 'grad_norm': 2.706472873687744, 'learning_rate': 1.488e-05, 'epoch': 0.7}\n",
      "{'loss': 0.2556, 'grad_norm': 4.448761940002441, 'learning_rate': 1.48e-05, 'epoch': 0.7}\n",
      "{'loss': 0.2779, 'grad_norm': 1.2970162630081177, 'learning_rate': 1.472e-05, 'epoch': 0.71}\n",
      "{'loss': 0.2221, 'grad_norm': 2.9856536388397217, 'learning_rate': 1.464e-05, 'epoch': 0.71}\n",
      "{'loss': 0.1934, 'grad_norm': 2.8866779804229736, 'learning_rate': 1.4560000000000001e-05, 'epoch': 0.71}\n",
      "{'loss': 0.1888, 'grad_norm': 1.089484453201294, 'learning_rate': 1.4480000000000002e-05, 'epoch': 0.71}\n",
      "{'loss': 0.2992, 'grad_norm': 2.286827564239502, 'learning_rate': 1.44e-05, 'epoch': 0.71}\n",
      "{'loss': 0.2637, 'grad_norm': 4.2568678855896, 'learning_rate': 1.432e-05, 'epoch': 0.71}\n",
      "{'loss': 0.2139, 'grad_norm': 2.53974986076355, 'learning_rate': 1.4240000000000001e-05, 'epoch': 0.72}\n",
      "{'loss': 0.0864, 'grad_norm': 1.9415652751922607, 'learning_rate': 1.4160000000000002e-05, 'epoch': 0.72}\n",
      "{'loss': 0.1893, 'grad_norm': 0.7902169823646545, 'learning_rate': 1.408e-05, 'epoch': 0.72}\n",
      "{'loss': 0.196, 'grad_norm': 3.3930246829986572, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.72}\n",
      "{'loss': 0.2089, 'grad_norm': 3.517042636871338, 'learning_rate': 1.3919999999999999e-05, 'epoch': 0.72}\n",
      "{'loss': 0.1364, 'grad_norm': 5.520136833190918, 'learning_rate': 1.384e-05, 'epoch': 0.72}\n",
      "{'loss': 0.2295, 'grad_norm': 3.384763479232788, 'learning_rate': 1.376e-05, 'epoch': 0.72}\n",
      "{'loss': 0.1819, 'grad_norm': 1.7641973495483398, 'learning_rate': 1.3680000000000001e-05, 'epoch': 0.73}\n",
      "{'loss': 0.2666, 'grad_norm': 6.447355270385742, 'learning_rate': 1.3600000000000002e-05, 'epoch': 0.73}\n",
      "{'loss': 0.248, 'grad_norm': 4.282230377197266, 'learning_rate': 1.352e-05, 'epoch': 0.73}\n",
      "{'loss': 0.1359, 'grad_norm': 1.2095036506652832, 'learning_rate': 1.344e-05, 'epoch': 0.73}\n",
      "{'loss': 0.2112, 'grad_norm': 3.4804582595825195, 'learning_rate': 1.336e-05, 'epoch': 0.73}\n",
      "{'loss': 0.202, 'grad_norm': 3.0156588554382324, 'learning_rate': 1.3280000000000002e-05, 'epoch': 0.73}\n",
      "{'loss': 0.2302, 'grad_norm': 3.0905823707580566, 'learning_rate': 1.32e-05, 'epoch': 0.74}\n",
      "{'loss': 0.2257, 'grad_norm': 2.107428550720215, 'learning_rate': 1.3120000000000001e-05, 'epoch': 0.74}\n",
      "{'loss': 0.2689, 'grad_norm': 2.106848955154419, 'learning_rate': 1.3039999999999999e-05, 'epoch': 0.74}\n",
      "{'loss': 0.2571, 'grad_norm': 5.270416736602783, 'learning_rate': 1.296e-05, 'epoch': 0.74}\n",
      "{'loss': 0.211, 'grad_norm': 2.8922057151794434, 'learning_rate': 1.288e-05, 'epoch': 0.74}\n",
      "{'loss': 0.2956, 'grad_norm': 5.870822906494141, 'learning_rate': 1.2800000000000001e-05, 'epoch': 0.74}\n",
      "{'loss': 0.1478, 'grad_norm': 0.6232583522796631, 'learning_rate': 1.2720000000000002e-05, 'epoch': 0.75}\n",
      "{'loss': 0.2257, 'grad_norm': 1.192568302154541, 'learning_rate': 1.2640000000000003e-05, 'epoch': 0.75}\n",
      "{'loss': 0.198, 'grad_norm': 2.245375156402588, 'learning_rate': 1.256e-05, 'epoch': 0.75}\n",
      "{'loss': 0.2012, 'grad_norm': 3.213338613510132, 'learning_rate': 1.248e-05, 'epoch': 0.75}\n",
      "{'loss': 0.2042, 'grad_norm': 1.6878904104232788, 'learning_rate': 1.24e-05, 'epoch': 0.75}\n",
      "{'loss': 0.2672, 'grad_norm': 3.940990924835205, 'learning_rate': 1.232e-05, 'epoch': 0.75}\n",
      "{'loss': 0.2424, 'grad_norm': 3.77883243560791, 'learning_rate': 1.224e-05, 'epoch': 0.76}\n",
      "{'loss': 0.2092, 'grad_norm': 2.7733051776885986, 'learning_rate': 1.216e-05, 'epoch': 0.76}\n",
      "{'loss': 0.2537, 'grad_norm': 3.5941264629364014, 'learning_rate': 1.2080000000000001e-05, 'epoch': 0.76}\n",
      "{'loss': 0.2479, 'grad_norm': 1.849347472190857, 'learning_rate': 1.2e-05, 'epoch': 0.76}\n",
      "{'loss': 0.1381, 'grad_norm': 3.5116336345672607, 'learning_rate': 1.1920000000000001e-05, 'epoch': 0.76}\n",
      "{'loss': 0.1746, 'grad_norm': 2.6792800426483154, 'learning_rate': 1.1840000000000002e-05, 'epoch': 0.76}\n",
      "{'loss': 0.138, 'grad_norm': 0.707854151725769, 'learning_rate': 1.1760000000000001e-05, 'epoch': 0.76}\n",
      "{'loss': 0.2583, 'grad_norm': 4.681334972381592, 'learning_rate': 1.168e-05, 'epoch': 0.77}\n",
      "{'loss': 0.1809, 'grad_norm': 2.7637195587158203, 'learning_rate': 1.16e-05, 'epoch': 0.77}\n",
      "{'loss': 0.1802, 'grad_norm': 3.5348451137542725, 'learning_rate': 1.152e-05, 'epoch': 0.77}\n",
      "{'loss': 0.1307, 'grad_norm': 2.4452974796295166, 'learning_rate': 1.144e-05, 'epoch': 0.77}\n",
      "{'loss': 0.2263, 'grad_norm': 2.0631022453308105, 'learning_rate': 1.1360000000000001e-05, 'epoch': 0.77}\n",
      "{'loss': 0.2922, 'grad_norm': 3.398895502090454, 'learning_rate': 1.128e-05, 'epoch': 0.77}\n",
      "{'loss': 0.1681, 'grad_norm': 2.7527008056640625, 'learning_rate': 1.1200000000000001e-05, 'epoch': 0.78}\n",
      "{'loss': 0.2715, 'grad_norm': 4.724951267242432, 'learning_rate': 1.112e-05, 'epoch': 0.78}\n",
      "{'loss': 0.2705, 'grad_norm': 2.855706214904785, 'learning_rate': 1.1040000000000001e-05, 'epoch': 0.78}\n",
      "{'loss': 0.1936, 'grad_norm': 1.294623851776123, 'learning_rate': 1.096e-05, 'epoch': 0.78}\n",
      "{'loss': 0.2368, 'grad_norm': 1.7508970499038696, 'learning_rate': 1.088e-05, 'epoch': 0.78}\n",
      "{'loss': 0.2063, 'grad_norm': 4.308847904205322, 'learning_rate': 1.08e-05, 'epoch': 0.78}\n",
      "{'loss': 0.2253, 'grad_norm': 3.309772491455078, 'learning_rate': 1.072e-05, 'epoch': 0.79}\n",
      "{'loss': 0.1466, 'grad_norm': 3.4909589290618896, 'learning_rate': 1.064e-05, 'epoch': 0.79}\n",
      "{'loss': 0.2327, 'grad_norm': 2.811689853668213, 'learning_rate': 1.056e-05, 'epoch': 0.79}\n",
      "{'loss': 0.2211, 'grad_norm': 2.052074670791626, 'learning_rate': 1.0480000000000001e-05, 'epoch': 0.79}\n",
      "{'loss': 0.1933, 'grad_norm': 0.43962016701698303, 'learning_rate': 1.04e-05, 'epoch': 0.79}\n",
      "{'loss': 0.2146, 'grad_norm': 1.3690236806869507, 'learning_rate': 1.0320000000000001e-05, 'epoch': 0.79}\n",
      "{'loss': 0.2165, 'grad_norm': 3.6177568435668945, 'learning_rate': 1.024e-05, 'epoch': 0.8}\n",
      "{'loss': 0.2499, 'grad_norm': 2.109949827194214, 'learning_rate': 1.016e-05, 'epoch': 0.8}\n",
      "{'loss': 0.3083, 'grad_norm': 4.510365009307861, 'learning_rate': 1.008e-05, 'epoch': 0.8}\n",
      "{'loss': 0.2159, 'grad_norm': 3.402857780456543, 'learning_rate': 1e-05, 'epoch': 0.8}\n",
      "{'loss': 0.1829, 'grad_norm': 3.464063882827759, 'learning_rate': 9.92e-06, 'epoch': 0.8}\n",
      "{'loss': 0.2269, 'grad_norm': 1.8900066614151, 'learning_rate': 9.84e-06, 'epoch': 0.8}\n",
      "{'loss': 0.2311, 'grad_norm': 3.45939040184021, 'learning_rate': 9.760000000000001e-06, 'epoch': 0.8}\n",
      "{'loss': 0.1981, 'grad_norm': 2.296794891357422, 'learning_rate': 9.68e-06, 'epoch': 0.81}\n",
      "{'loss': 0.1707, 'grad_norm': 1.1320469379425049, 'learning_rate': 9.600000000000001e-06, 'epoch': 0.81}\n",
      "{'loss': 0.3009, 'grad_norm': 6.815478801727295, 'learning_rate': 9.52e-06, 'epoch': 0.81}\n",
      "{'loss': 0.2901, 'grad_norm': 1.641756534576416, 'learning_rate': 9.44e-06, 'epoch': 0.81}\n",
      "{'loss': 0.2135, 'grad_norm': 3.499042510986328, 'learning_rate': 9.36e-06, 'epoch': 0.81}\n",
      "{'loss': 0.26, 'grad_norm': 3.252045154571533, 'learning_rate': 9.28e-06, 'epoch': 0.81}\n",
      "{'loss': 0.15, 'grad_norm': 1.129459023475647, 'learning_rate': 9.2e-06, 'epoch': 0.82}\n",
      "{'loss': 0.2087, 'grad_norm': 1.4624119997024536, 'learning_rate': 9.12e-06, 'epoch': 0.82}\n",
      "{'loss': 0.1869, 'grad_norm': 0.9124441742897034, 'learning_rate': 9.04e-06, 'epoch': 0.82}\n",
      "{'loss': 0.3383, 'grad_norm': 4.6462202072143555, 'learning_rate': 8.96e-06, 'epoch': 0.82}\n",
      "{'loss': 0.1967, 'grad_norm': 4.879650115966797, 'learning_rate': 8.880000000000001e-06, 'epoch': 0.82}\n",
      "{'loss': 0.3729, 'grad_norm': 1.7998390197753906, 'learning_rate': 8.8e-06, 'epoch': 0.82}\n",
      "{'loss': 0.162, 'grad_norm': 0.8473380208015442, 'learning_rate': 8.720000000000001e-06, 'epoch': 0.83}\n",
      "{'loss': 0.2494, 'grad_norm': 4.540779113769531, 'learning_rate': 8.64e-06, 'epoch': 0.83}\n",
      "{'loss': 0.2988, 'grad_norm': 1.6424920558929443, 'learning_rate': 8.56e-06, 'epoch': 0.83}\n",
      "{'loss': 0.1776, 'grad_norm': 4.756498336791992, 'learning_rate': 8.48e-06, 'epoch': 0.83}\n",
      "{'loss': 0.1551, 'grad_norm': 5.164284706115723, 'learning_rate': 8.400000000000001e-06, 'epoch': 0.83}\n",
      "{'loss': 0.273, 'grad_norm': 4.170656681060791, 'learning_rate': 8.32e-06, 'epoch': 0.83}\n",
      "{'loss': 0.1652, 'grad_norm': 2.7148277759552, 'learning_rate': 8.24e-06, 'epoch': 0.84}\n",
      "{'loss': 0.1999, 'grad_norm': 2.247673511505127, 'learning_rate': 8.160000000000001e-06, 'epoch': 0.84}\n",
      "{'loss': 0.1786, 'grad_norm': 3.1619060039520264, 'learning_rate': 8.08e-06, 'epoch': 0.84}\n",
      "{'loss': 0.172, 'grad_norm': 5.092113971710205, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.84}\n",
      "{'loss': 0.2525, 'grad_norm': 2.2226991653442383, 'learning_rate': 7.92e-06, 'epoch': 0.84}\n",
      "{'loss': 0.3228, 'grad_norm': 5.031032562255859, 'learning_rate': 7.84e-06, 'epoch': 0.84}\n",
      "{'loss': 0.2003, 'grad_norm': 3.7319581508636475, 'learning_rate': 7.76e-06, 'epoch': 0.84}\n",
      "{'loss': 0.1883, 'grad_norm': 1.9255669116973877, 'learning_rate': 7.68e-06, 'epoch': 0.85}\n",
      "{'loss': 0.2273, 'grad_norm': 2.0031723976135254, 'learning_rate': 7.6e-06, 'epoch': 0.85}\n",
      "{'loss': 0.2103, 'grad_norm': 2.4879300594329834, 'learning_rate': 7.520000000000001e-06, 'epoch': 0.85}\n",
      "{'loss': 0.2389, 'grad_norm': 1.5577797889709473, 'learning_rate': 7.44e-06, 'epoch': 0.85}\n",
      "{'loss': 0.2555, 'grad_norm': 4.606823921203613, 'learning_rate': 7.36e-06, 'epoch': 0.85}\n",
      "{'loss': 0.1375, 'grad_norm': 2.531902313232422, 'learning_rate': 7.280000000000001e-06, 'epoch': 0.85}\n",
      "{'loss': 0.1637, 'grad_norm': 2.661672830581665, 'learning_rate': 7.2e-06, 'epoch': 0.86}\n",
      "{'loss': 0.1658, 'grad_norm': 3.2737555503845215, 'learning_rate': 7.1200000000000004e-06, 'epoch': 0.86}\n",
      "{'loss': 0.1764, 'grad_norm': 2.2581284046173096, 'learning_rate': 7.04e-06, 'epoch': 0.86}\n",
      "{'loss': 0.1968, 'grad_norm': 2.030418872833252, 'learning_rate': 6.9599999999999994e-06, 'epoch': 0.86}\n",
      "{'loss': 0.1992, 'grad_norm': 3.053710460662842, 'learning_rate': 6.88e-06, 'epoch': 0.86}\n",
      "{'loss': 0.2043, 'grad_norm': 3.2392656803131104, 'learning_rate': 6.800000000000001e-06, 'epoch': 0.86}\n",
      "{'loss': 0.1469, 'grad_norm': 3.1546573638916016, 'learning_rate': 6.72e-06, 'epoch': 0.87}\n",
      "{'loss': 0.248, 'grad_norm': 2.5030629634857178, 'learning_rate': 6.640000000000001e-06, 'epoch': 0.87}\n",
      "{'loss': 0.1481, 'grad_norm': 3.48970627784729, 'learning_rate': 6.560000000000001e-06, 'epoch': 0.87}\n",
      "{'loss': 0.2475, 'grad_norm': 2.0191915035247803, 'learning_rate': 6.48e-06, 'epoch': 0.87}\n",
      "{'loss': 0.164, 'grad_norm': 1.2796432971954346, 'learning_rate': 6.4000000000000006e-06, 'epoch': 0.87}\n",
      "{'loss': 0.2957, 'grad_norm': 3.534003973007202, 'learning_rate': 6.320000000000001e-06, 'epoch': 0.87}\n",
      "{'loss': 0.2384, 'grad_norm': 6.204458236694336, 'learning_rate': 6.24e-06, 'epoch': 0.88}\n",
      "{'loss': 0.2393, 'grad_norm': 2.4614169597625732, 'learning_rate': 6.16e-06, 'epoch': 0.88}\n",
      "{'loss': 0.1246, 'grad_norm': 1.5514369010925293, 'learning_rate': 6.08e-06, 'epoch': 0.88}\n",
      "{'loss': 0.2939, 'grad_norm': 3.0430593490600586, 'learning_rate': 6e-06, 'epoch': 0.88}\n",
      "{'loss': 0.2351, 'grad_norm': 0.44382554292678833, 'learning_rate': 5.920000000000001e-06, 'epoch': 0.88}\n",
      "{'loss': 0.2549, 'grad_norm': 4.912315368652344, 'learning_rate': 5.84e-06, 'epoch': 0.88}\n",
      "{'loss': 0.269, 'grad_norm': 4.88657283782959, 'learning_rate': 5.76e-06, 'epoch': 0.88}\n",
      "{'loss': 0.3053, 'grad_norm': 3.4496469497680664, 'learning_rate': 5.680000000000001e-06, 'epoch': 0.89}\n",
      "{'loss': 0.1796, 'grad_norm': 1.597140908241272, 'learning_rate': 5.600000000000001e-06, 'epoch': 0.89}\n",
      "{'loss': 0.2858, 'grad_norm': 4.801187515258789, 'learning_rate': 5.5200000000000005e-06, 'epoch': 0.89}\n",
      "{'loss': 0.1373, 'grad_norm': 1.8374483585357666, 'learning_rate': 5.44e-06, 'epoch': 0.89}\n",
      "{'loss': 0.2285, 'grad_norm': 1.5115724802017212, 'learning_rate': 5.36e-06, 'epoch': 0.89}\n",
      "{'loss': 0.181, 'grad_norm': 3.564091205596924, 'learning_rate': 5.28e-06, 'epoch': 0.89}\n",
      "{'loss': 0.1915, 'grad_norm': 2.0685925483703613, 'learning_rate': 5.2e-06, 'epoch': 0.9}\n",
      "{'loss': 0.2797, 'grad_norm': 3.8730902671813965, 'learning_rate': 5.12e-06, 'epoch': 0.9}\n",
      "{'loss': 0.204, 'grad_norm': 2.6573593616485596, 'learning_rate': 5.04e-06, 'epoch': 0.9}\n",
      "{'loss': 0.2623, 'grad_norm': 2.2311480045318604, 'learning_rate': 4.96e-06, 'epoch': 0.9}\n",
      "{'loss': 0.2402, 'grad_norm': 0.6449623107910156, 'learning_rate': 4.880000000000001e-06, 'epoch': 0.9}\n",
      "{'loss': 0.298, 'grad_norm': 3.5991814136505127, 'learning_rate': 4.800000000000001e-06, 'epoch': 0.9}\n",
      "{'loss': 0.1823, 'grad_norm': 2.675107002258301, 'learning_rate': 4.72e-06, 'epoch': 0.91}\n",
      "{'loss': 0.2142, 'grad_norm': 1.5227408409118652, 'learning_rate': 4.64e-06, 'epoch': 0.91}\n",
      "{'loss': 0.2874, 'grad_norm': 2.073904037475586, 'learning_rate': 4.56e-06, 'epoch': 0.91}\n",
      "{'loss': 0.2333, 'grad_norm': 1.605431079864502, 'learning_rate': 4.48e-06, 'epoch': 0.91}\n",
      "{'loss': 0.2313, 'grad_norm': 1.9658658504486084, 'learning_rate': 4.4e-06, 'epoch': 0.91}\n",
      "{'loss': 0.2035, 'grad_norm': 4.181090831756592, 'learning_rate': 4.32e-06, 'epoch': 0.91}\n",
      "{'loss': 0.2335, 'grad_norm': 3.4441072940826416, 'learning_rate': 4.24e-06, 'epoch': 0.92}\n",
      "{'loss': 0.1709, 'grad_norm': 0.7324643731117249, 'learning_rate': 4.16e-06, 'epoch': 0.92}\n",
      "{'loss': 0.2086, 'grad_norm': 2.9708569049835205, 'learning_rate': 4.080000000000001e-06, 'epoch': 0.92}\n",
      "{'loss': 0.2833, 'grad_norm': 3.2798824310302734, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.92}\n",
      "{'loss': 0.236, 'grad_norm': 3.1843814849853516, 'learning_rate': 3.92e-06, 'epoch': 0.92}\n",
      "{'loss': 0.1639, 'grad_norm': 3.1793956756591797, 'learning_rate': 3.84e-06, 'epoch': 0.92}\n",
      "{'loss': 0.2557, 'grad_norm': 3.78702449798584, 'learning_rate': 3.7600000000000004e-06, 'epoch': 0.92}\n",
      "{'loss': 0.2241, 'grad_norm': 0.7672680020332336, 'learning_rate': 3.68e-06, 'epoch': 0.93}\n",
      "{'loss': 0.2682, 'grad_norm': 5.374373912811279, 'learning_rate': 3.6e-06, 'epoch': 0.93}\n",
      "{'loss': 0.2658, 'grad_norm': 3.1638944149017334, 'learning_rate': 3.52e-06, 'epoch': 0.93}\n",
      "{'loss': 0.142, 'grad_norm': 1.2732754945755005, 'learning_rate': 3.44e-06, 'epoch': 0.93}\n",
      "{'loss': 0.2073, 'grad_norm': 1.315025806427002, 'learning_rate': 3.36e-06, 'epoch': 0.93}\n",
      "{'loss': 0.2125, 'grad_norm': 2.904179334640503, 'learning_rate': 3.2800000000000004e-06, 'epoch': 0.93}\n",
      "{'loss': 0.2149, 'grad_norm': 1.150330662727356, 'learning_rate': 3.2000000000000003e-06, 'epoch': 0.94}\n",
      "{'loss': 0.2369, 'grad_norm': 1.9597417116165161, 'learning_rate': 3.12e-06, 'epoch': 0.94}\n",
      "{'loss': 0.2238, 'grad_norm': 3.1151504516601562, 'learning_rate': 3.04e-06, 'epoch': 0.94}\n",
      "{'loss': 0.2186, 'grad_norm': 2.477595806121826, 'learning_rate': 2.9600000000000005e-06, 'epoch': 0.94}\n",
      "{'loss': 0.1689, 'grad_norm': 2.0696098804473877, 'learning_rate': 2.88e-06, 'epoch': 0.94}\n",
      "{'loss': 0.1143, 'grad_norm': 2.339515209197998, 'learning_rate': 2.8000000000000003e-06, 'epoch': 0.94}\n",
      "{'loss': 0.3427, 'grad_norm': 2.984382390975952, 'learning_rate': 2.72e-06, 'epoch': 0.95}\n",
      "{'loss': 0.1517, 'grad_norm': 1.3338629007339478, 'learning_rate': 2.64e-06, 'epoch': 0.95}\n",
      "{'loss': 0.1418, 'grad_norm': 1.1301836967468262, 'learning_rate': 2.56e-06, 'epoch': 0.95}\n",
      "{'loss': 0.1887, 'grad_norm': 2.536302328109741, 'learning_rate': 2.48e-06, 'epoch': 0.95}\n",
      "{'loss': 0.2684, 'grad_norm': 3.271496295928955, 'learning_rate': 2.4000000000000003e-06, 'epoch': 0.95}\n",
      "{'loss': 0.1905, 'grad_norm': 3.4846997261047363, 'learning_rate': 2.32e-06, 'epoch': 0.95}\n",
      "{'loss': 0.2013, 'grad_norm': 1.2230784893035889, 'learning_rate': 2.24e-06, 'epoch': 0.96}\n",
      "{'loss': 0.217, 'grad_norm': 4.204478740692139, 'learning_rate': 2.16e-06, 'epoch': 0.96}\n",
      "{'loss': 0.333, 'grad_norm': 3.8360936641693115, 'learning_rate': 2.08e-06, 'epoch': 0.96}\n",
      "{'loss': 0.178, 'grad_norm': 1.358791708946228, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.96}\n",
      "{'loss': 0.1556, 'grad_norm': 6.027934551239014, 'learning_rate': 1.92e-06, 'epoch': 0.96}\n",
      "{'loss': 0.3044, 'grad_norm': 1.1073930263519287, 'learning_rate': 1.84e-06, 'epoch': 0.96}\n",
      "{'loss': 0.1458, 'grad_norm': 2.1369504928588867, 'learning_rate': 1.76e-06, 'epoch': 0.96}\n",
      "{'loss': 0.2274, 'grad_norm': 2.6137900352478027, 'learning_rate': 1.68e-06, 'epoch': 0.97}\n",
      "{'loss': 0.2046, 'grad_norm': 1.953824758529663, 'learning_rate': 1.6000000000000001e-06, 'epoch': 0.97}\n",
      "{'loss': 0.1975, 'grad_norm': 2.3091328144073486, 'learning_rate': 1.52e-06, 'epoch': 0.97}\n",
      "{'loss': 0.2244, 'grad_norm': 2.242903709411621, 'learning_rate': 1.44e-06, 'epoch': 0.97}\n",
      "{'loss': 0.2064, 'grad_norm': 5.070217609405518, 'learning_rate': 1.36e-06, 'epoch': 0.97}\n",
      "{'loss': 0.259, 'grad_norm': 1.8023285865783691, 'learning_rate': 1.28e-06, 'epoch': 0.97}\n",
      "{'loss': 0.2142, 'grad_norm': 4.498645782470703, 'learning_rate': 1.2000000000000002e-06, 'epoch': 0.98}\n",
      "{'loss': 0.2316, 'grad_norm': 1.2322731018066406, 'learning_rate': 1.12e-06, 'epoch': 0.98}\n",
      "{'loss': 0.2753, 'grad_norm': 4.607349872589111, 'learning_rate': 1.04e-06, 'epoch': 0.98}\n",
      "{'loss': 0.1851, 'grad_norm': 2.1617684364318848, 'learning_rate': 9.6e-07, 'epoch': 0.98}\n",
      "{'loss': 0.2683, 'grad_norm': 2.4536619186401367, 'learning_rate': 8.8e-07, 'epoch': 0.98}\n",
      "{'loss': 0.259, 'grad_norm': 3.1160621643066406, 'learning_rate': 8.000000000000001e-07, 'epoch': 0.98}\n",
      "{'loss': 0.2363, 'grad_norm': 0.5285778045654297, 'learning_rate': 7.2e-07, 'epoch': 0.99}\n",
      "{'loss': 0.2346, 'grad_norm': 3.1239261627197266, 'learning_rate': 6.4e-07, 'epoch': 0.99}\n",
      "{'loss': 0.2114, 'grad_norm': 2.727216958999634, 'learning_rate': 5.6e-07, 'epoch': 0.99}\n",
      "{'loss': 0.1471, 'grad_norm': 1.2235908508300781, 'learning_rate': 4.8e-07, 'epoch': 0.99}\n",
      "{'loss': 0.2927, 'grad_norm': 3.4679174423217773, 'learning_rate': 4.0000000000000003e-07, 'epoch': 0.99}\n",
      "{'loss': 0.2247, 'grad_norm': 1.3873989582061768, 'learning_rate': 3.2e-07, 'epoch': 0.99}\n",
      "{'loss': 0.1618, 'grad_norm': 2.769379138946533, 'learning_rate': 2.4e-07, 'epoch': 1.0}\n",
      "{'loss': 0.2546, 'grad_norm': 5.319918155670166, 'learning_rate': 1.6e-07, 'epoch': 1.0}\n",
      "{'loss': 0.2343, 'grad_norm': 2.07857346534729, 'learning_rate': 8e-08, 'epoch': 1.0}\n",
      "{'loss': 0.2707, 'grad_norm': 2.3043136596679688, 'learning_rate': 0.0, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a9a0f7ec61e4e0fb7b76a285e988edf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.21493571996688843, 'eval_model_preparation_time': 0.003, 'eval_accuracy': 0.91912, 'eval_precision': 0.9257195914577531, 'eval_recall': 0.9123135070618659, 'eval_f1': 0.9189676591992947, 'eval_runtime': 87.2417, 'eval_samples_per_second': 573.12, 'eval_steps_per_second': 35.82, 'epoch': 1.0}\n",
      "{'train_runtime': 419.4052, 'train_samples_per_second': 238.433, 'train_steps_per_second': 14.902, 'train_loss': 0.24306428087234497, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6250, training_loss=0.24306428087234497, metrics={'train_runtime': 419.4052, 'train_samples_per_second': 238.433, 'train_steps_per_second': 14.902, 'total_flos': 6804918067200000.0, 'train_loss': 0.24306428087234497, 'epoch': 1.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the LoRA model (Step 10)\n",
    "trainer.model = peft_model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c4d4c908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model (Step 11)\n",
    "peft_model.save_pretrained(\"./lora_distilbert_amazon_polarity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b12c6",
   "metadata": {},
   "source": [
    "## Performing Inference with a PEFT Model\n",
    "\n",
    "TODO: In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cbf0cac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): DistilBertForSequenceClassification(\n",
       "      (distilbert): DistilBertModel(\n",
       "        (embeddings): Embeddings(\n",
       "          (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "          (position_embeddings): Embedding(512, 768)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (transformer): Transformer(\n",
       "          (layer): ModuleList(\n",
       "            (0-5): 6 x TransformerBlock(\n",
       "              (attention): DistilBertSdpaAttention(\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (q_lin): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (v_lin): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              )\n",
       "              (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (ffn): FFN(\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (activation): GELUActivation()\n",
       "              )\n",
       "              (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pre_classifier): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (classifier): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=768, out_features=2, bias=True)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=768, out_features=2, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the fine-tuned model for inference (Step 12)\n",
    "from peft import AutoPeftModelForSequenceClassification\n",
    "fine_tuned_model = AutoPeftModelForSequenceClassification.from_pretrained(\n",
    "    \"./lora_distilbert_amazon_polarity\", config=config\n",
    ")\n",
    "fine_tuned_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "trainer.model = fine_tuned_model\n",
    "trainer.model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bc3a8147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the fine-tuned model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81b88161d91c49a3bc71b0a1e640eca0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-Tuned Model Results: {'eval_loss': 0.21493571996688843, 'eval_model_preparation_time': 0.003, 'eval_accuracy': 0.91912, 'eval_precision': 0.9257195914577531, 'eval_recall': 0.9123135070618659, 'eval_f1': 0.9189676591992947, 'eval_runtime': 86.3589, 'eval_samples_per_second': 578.979, 'eval_steps_per_second': 36.186, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the fine-tuned model (Step 13)\n",
    "print(\"Evaluating the fine-tuned model...\")\n",
    "fine_tuned_results = trainer.evaluate()\n",
    "print(\"Fine-Tuned Model Results:\", fine_tuned_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "866ab28c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparison:\n",
      "Metric         Base Model          Fine-Tuned Model    \n",
      "-------------------------------------------------------\n",
      "Accuracy       0.5036              0.91912             \n",
      "Precision      0.5036159515118122  0.9257195914577531  \n",
      "Recall         0.8727272727272727  0.9123135070618659  \n",
      "F1 Score       0.6386769929540558  0.9189676591992947  \n"
     ]
    }
   ],
   "source": [
    "# Comparison of Results (Step 14)\n",
    "print(\"\\nComparison:\")\n",
    "print(f\"{'Metric':<15}{'Base Model':<20}{'Fine-Tuned Model':<20}\")\n",
    "print(f\"{'-'*55}\")\n",
    "print(f\"{'Accuracy':<15}{base_results['eval_accuracy']:<20}{fine_tuned_results['eval_accuracy']:<20}\")\n",
    "print(f\"{'Precision':<15}{base_results['eval_precision']:<20}{fine_tuned_results['eval_precision']:<20}\")\n",
    "print(f\"{'Recall':<15}{base_results['eval_recall']:<20}{fine_tuned_results['eval_recall']:<20}\")\n",
    "print(f\"{'F1 Score':<15}{base_results['eval_f1']:<20}{fine_tuned_results['eval_f1']:<20}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8230ddbc",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The results demonstrate a significant improvement in performance after fine-tuning the DistilBERT model using the LoRA technique on the Amazon Polarity dataset:\n",
    "\n",
    "Accuracy: The fine-tuned model achieved an accuracy of 91.9%, a substantial improvement over the base model's 50.36%, indicating that the model effectively learned the task during fine-tuning.\n",
    "\n",
    "Precision: The precision increased from 50.361% to 92.57%, showing that the fine-tuned model is much better at correctly identifying positive predictions without false positives.\n",
    "\n",
    "Recall: Recall improved from 87.2% to 91.2%, suggesting that the fine-tuned model successfully captures most of the true positives in the dataset.\n",
    "\n",
    "F1 Score: The F1 score, which balances precision and recall, rose dramatically from 63.8% to 91.9%, highlighting an improvement in the model's classification ability.\n",
    "By focusing on training only a small number of parameters, this approach achieves high performance with reduced computational resources, making it ideal for deployment in resource-constrained environments."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyt312Cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
